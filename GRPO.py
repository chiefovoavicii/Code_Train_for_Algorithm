"""
GRPO (Group Relative Policy Optimization) æœ€å°å®ç°
ç”¨äºè¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œæ˜¯PPOçš„æ”¹è¿›ç‰ˆæœ¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
from typing import List, Tuple, Dict


class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼ˆè¯­è¨€æ¨¡å‹ï¼‰"""
    
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):
        """
        å‚æ•°:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embed_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            num_layers: LSTMå±‚æ•°
        """
        super(PolicyNetwork, self).__init__()
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTMä½œä¸ºè¯­è¨€æ¨¡å‹éª¨å¹²
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)
        
        # è¾“å‡ºå±‚ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡ï¼‰
        self.fc_out = nn.Linear(hidden_dim, vocab_size)
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
    def forward(self, x, hidden=None):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]
            hidden: LSTMéšè—çŠ¶æ€
        è¿”å›:
            logits: è¾“å‡ºlogits [batch_size, seq_len, vocab_size]
            hidden: æ–°çš„éšè—çŠ¶æ€
        """
        # è¯åµŒå…¥
        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        
        # LSTMå‰å‘ä¼ æ’­
        if hidden is None:
            lstm_out, hidden = self.lstm(embedded)
        else:
            lstm_out, hidden = self.lstm(embedded, hidden)
        
        # è¾“å‡ºå±‚
        logits = self.fc_out(lstm_out)  # [batch_size, seq_len, vocab_size]
        
        return logits, hidden
    
    def get_log_probs(self, sequences):
        """
        è®¡ç®—åºåˆ—çš„å¯¹æ•°æ¦‚ç‡
        å‚æ•°:
            sequences: tokenåºåˆ— [batch_size, seq_len]
        è¿”å›:
            log_probs: æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ [batch_size, seq_len-1]
        """
        # è¾“å…¥æ˜¯sequences[:-1]ï¼Œç›®æ ‡æ˜¯sequences[1:]
        inputs = sequences[:, :-1]
        targets = sequences[:, 1:]
        
        # å‰å‘ä¼ æ’­
        logits, _ = self.forward(inputs)  # [batch_size, seq_len-1, vocab_size]
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        log_probs = F.log_softmax(logits, dim=-1)
        
        # æ”¶é›†ç›®æ ‡tokençš„å¯¹æ•°æ¦‚ç‡
        target_log_probs = torch.gather(
            log_probs, 
            dim=-1, 
            index=targets.unsqueeze(-1)
        ).squeeze(-1)  # [batch_size, seq_len-1]
        
        return target_log_probs
    
    def generate(self, start_tokens, max_length=20, temperature=1.0):
        """
        ç”Ÿæˆæ–‡æœ¬åºåˆ—
        å‚æ•°:
            start_tokens: èµ·å§‹token [batch_size, start_len]
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
        è¿”å›:
            generated: ç”Ÿæˆçš„å®Œæ•´åºåˆ—
            log_probs: ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯¹æ•°æ¦‚ç‡
        """
        batch_size = start_tokens.size(0)
        generated = start_tokens.clone()
        log_probs_list = []
        hidden = None
        
        for _ in range(max_length):
            # è·å–æœ€åä¸€ä¸ªtokençš„logits
            logits, hidden = self.forward(generated[:, -1:], hidden)
            logits = logits[:, -1, :] / temperature  # [batch_size, vocab_size]
            
            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            dist = Categorical(probs)
            next_token = dist.sample()  # [batch_size]
            
            # è®°å½•å¯¹æ•°æ¦‚ç‡
            log_prob = dist.log_prob(next_token)
            log_probs_list.append(log_prob)
            
            # æ‹¼æ¥åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token.unsqueeze(1)], dim=1)
        
        log_probs = torch.stack(log_probs_list, dim=1)  # [batch_size, max_length]
        
        return generated, log_probs


class ValueNetwork(nn.Module):
    """ä»·å€¼ç½‘ç»œï¼ˆè¯„ä¼°çŠ¶æ€ä»·å€¼ï¼‰"""
    
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):
        """
        å‚æ•°:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embed_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            num_layers: LSTMå±‚æ•°
        """
        super(ValueNetwork, self).__init__()
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTM
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)
        
        # è¾“å‡ºå±‚ï¼ˆé¢„æµ‹ä»·å€¼ï¼‰
        self.fc_out = nn.Linear(hidden_dim, 1)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]
        è¿”å›:
            values: çŠ¶æ€ä»·å€¼ [batch_size, seq_len]
        """
        # è¯åµŒå…¥
        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim]
        
        # è¾“å‡ºä»·å€¼
        values = self.fc_out(lstm_out).squeeze(-1)  # [batch_size, seq_len]
        
        return values


class RewardModel(nn.Module):
    """å¥–åŠ±æ¨¡å‹ï¼ˆè¯„ä¼°ç”Ÿæˆè´¨é‡ï¼‰"""
    
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):
        """
        å‚æ•°:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embed_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(RewardModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
        
    def forward(self, sequences):
        """
        è®¡ç®—åºåˆ—çš„å¥–åŠ±åˆ†æ•°
        å‚æ•°:
            sequences: tokenåºåˆ— [batch_size, seq_len]
        è¿”å›:
            rewards: å¥–åŠ±åˆ†æ•° [batch_size]
        """
        embedded = self.embedding(sequences)
        lstm_out, (h_n, _) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªéšè—çŠ¶æ€
        reward = self.fc(h_n[-1]).squeeze(-1)  # [batch_size]
        
        return reward


class GRPO:
    """
    Group Relative Policy Optimization
    ç›¸æ¯”PPOçš„æ”¹è¿›ï¼šä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼Œè€Œä¸æ˜¯å…¨å±€ä¼˜åŠ¿
    """
    
    def __init__(
        self,
        policy: PolicyNetwork,
        value_net: ValueNetwork,
        reward_model: RewardModel,
        lr_policy=1e-4,
        lr_value=1e-4,
        gamma=0.99,
        lambda_gae=0.95,
        epsilon_clip=0.2,
        num_epochs=4,
        group_size=4,
        kl_coef=0.1
    ):
        """
        å‚æ•°:
            policy: ç­–ç•¥ç½‘ç»œ
            value_net: ä»·å€¼ç½‘ç»œ
            reward_model: å¥–åŠ±æ¨¡å‹
            lr_policy: ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡
            lr_value: ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            lambda_gae: GAEå‚æ•°
            epsilon_clip: PPOè£å‰ªå‚æ•°
            num_epochs: æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
            group_size: ç»„å¤§å°ï¼ˆGRPOæ ¸å¿ƒå‚æ•°ï¼‰
            kl_coef: KLæ•£åº¦æƒ©ç½šç³»æ•°
        """
        self.policy = policy
        self.value_net = value_net
        self.reward_model = reward_model
        
        # ä¼˜åŒ–å™¨
        self.policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr_policy)
        self.value_optimizer = torch.optim.Adam(value_net.parameters(), lr=lr_value)
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.epsilon_clip = epsilon_clip
        self.num_epochs = num_epochs
        self.group_size = group_size
        self.kl_coef = kl_coef
        
        # ä¿å­˜å‚è€ƒç­–ç•¥ï¼ˆç”¨äºKLæ•£åº¦è®¡ç®—ï¼‰
        self.ref_policy = None
        
    def compute_gae(self, rewards, values, dones):
        """
        è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(Generalized Advantage Estimation)
        å‚æ•°:
            rewards: å¥–åŠ± [batch_size, seq_len]
            values: ä»·å€¼ä¼°è®¡ [batch_size, seq_len]
            dones: ç»“æŸæ ‡å¿— [batch_size, seq_len]
        è¿”å›:
            advantages: ä¼˜åŠ¿å‡½æ•° [batch_size, seq_len]
            returns: å›æŠ¥ [batch_size, seq_len]
        """
        batch_size, seq_len = rewards.shape
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)
        
        gae = 0
        for t in reversed(range(seq_len)):
            if t == seq_len - 1:
                next_value = 0
            else:
                next_value = values[:, t + 1]
            
            # TDè¯¯å·®
            delta = rewards[:, t] + self.gamma * next_value * (1 - dones[:, t]) - values[:, t]
            
            # GAE
            gae = delta + self.gamma * self.lambda_gae * (1 - dones[:, t]) * gae
            advantages[:, t] = gae
            returns[:, t] = advantages[:, t] + values[:, t]
        
        return advantages, returns
    
    def compute_group_advantages(self, advantages, group_size):
        """
        è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆGRPOçš„æ ¸å¿ƒåˆ›æ–°ï¼‰
        å‚æ•°:
            advantages: åŸå§‹ä¼˜åŠ¿ [batch_size, seq_len]
            group_size: ç»„å¤§å°
        è¿”å›:
            group_advantages: ç»„å†…å½’ä¸€åŒ–çš„ä¼˜åŠ¿
        """
        batch_size, seq_len = advantages.shape
        
        # å°†batchåˆ†ç»„
        num_groups = batch_size // group_size
        advantages_reshaped = advantages[:num_groups * group_size].view(num_groups, group_size, seq_len)
        
        # ç»„å†…æ ‡å‡†åŒ–ï¼ˆå‡å»ç»„å‡å€¼ï¼Œé™¤ä»¥ç»„æ ‡å‡†å·®ï¼‰
        group_mean = advantages_reshaped.mean(dim=1, keepdim=True)
        group_std = advantages_reshaped.std(dim=1, keepdim=True) + 1e-8
        
        group_advantages = (advantages_reshaped - group_mean) / group_std
        group_advantages = group_advantages.view(-1, seq_len)
        
        # å¤„ç†å‰©ä½™æ ·æœ¬ï¼ˆå¦‚æœbatch_sizeä¸èƒ½è¢«group_sizeæ•´é™¤ï¼‰
        if batch_size % group_size != 0:
            remaining = advantages[num_groups * group_size:]
            remaining_normalized = (remaining - remaining.mean()) / (remaining.std() + 1e-8)
            group_advantages = torch.cat([group_advantages, remaining_normalized], dim=0)
        
        return group_advantages
    
    def ppo_loss(self, old_log_probs, new_log_probs, advantages, epsilon):
        """
        è®¡ç®—PPOæŸå¤±
        å‚æ•°:
            old_log_probs: æ—§ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            new_log_probs: æ–°ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            advantages: ä¼˜åŠ¿å‡½æ•°
            epsilon: è£å‰ªå‚æ•°
        è¿”å›:
            loss: PPOæŸå¤±
        """
        # è®¡ç®—æ¦‚ç‡æ¯”
        ratio = torch.exp(new_log_probs - old_log_probs)
        
        # æœªè£å‰ªçš„ç›®æ ‡
        surr1 = ratio * advantages
        
        # è£å‰ªçš„ç›®æ ‡
        surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages
        
        # PPOæŸå¤±ï¼ˆå–æœ€å°å€¼ï¼Œå³æœ€ä¿å®ˆçš„æ›´æ–°ï¼‰
        loss = -torch.min(surr1, surr2).mean()
        
        return loss
    
    def compute_kl_divergence(self, sequences, old_log_probs):
        """
        è®¡ç®—KLæ•£åº¦ï¼ˆé˜²æ­¢ç­–ç•¥åç¦»å¤ªè¿œï¼‰
        å‚æ•°:
            sequences: tokenåºåˆ—
            old_log_probs: å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
        è¿”å›:
            kl_div: KLæ•£åº¦
        """
        # è·å–å½“å‰ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
        new_log_probs = self.policy.get_log_probs(sequences)
        
        # KLæ•£åº¦: KL(old||new) = old_log_probs - new_log_probs
        kl_div = (old_log_probs - new_log_probs).mean()
        
        return kl_div
    
    def train_step(self, prompts, generated_sequences, rewards_scores):
        """
        æ‰§è¡Œä¸€æ­¥GRPOè®­ç»ƒ
        å‚æ•°:
            prompts: æç¤ºåºåˆ— [batch_size, prompt_len]
            generated_sequences: ç”Ÿæˆçš„å®Œæ•´åºåˆ— [batch_size, total_len]
            rewards_scores: å¥–åŠ±åˆ†æ•° [batch_size]
        è¿”å›:
            metrics: è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        batch_size = generated_sequences.size(0)
        seq_len = generated_sequences.size(1) - 1  # å‡1å› ä¸ºè¦é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
        
        # 1. è®¡ç®—æ—§ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡ï¼ˆç”¨äºPPOï¼‰
        with torch.no_grad():
            old_log_probs = self.policy.get_log_probs(generated_sequences)
        
        # 2. è®¡ç®—ä»·å€¼ä¼°è®¡
        with torch.no_grad():
            values = self.value_net(generated_sequences[:, :-1])
        
        # 3. æ„å»ºå¥–åŠ±ï¼ˆåªåœ¨åºåˆ—æœ«å°¾ç»™å¥–åŠ±ï¼‰
        rewards = torch.zeros(batch_size, seq_len, device=generated_sequences.device)
        rewards[:, -1] = rewards_scores  # æœ€åä¸€ä¸ªä½ç½®è·å¾—å¥–åŠ±
        
        # 4. è®¡ç®—GAEä¼˜åŠ¿
        dones = torch.zeros(batch_size, seq_len, device=generated_sequences.device)
        dones[:, -1] = 1.0  # åºåˆ—ç»“æŸ
        
        advantages, returns = self.compute_gae(rewards, values, dones)
        
        # 5. è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆGRPOæ ¸å¿ƒï¼‰
        group_advantages = self.compute_group_advantages(advantages, self.group_size)
        
        # 6. å¤šè½®æ›´æ–°
        metrics = {
            'policy_loss': 0,
            'value_loss': 0,
            'kl_div': 0,
            'entropy': 0
        }
        
        for epoch in range(self.num_epochs):
            # æ›´æ–°ç­–ç•¥ç½‘ç»œ
            self.policy_optimizer.zero_grad()
            
            # è·å–æ–°çš„å¯¹æ•°æ¦‚ç‡
            new_log_probs = self.policy.get_log_probs(generated_sequences)
            
            # è®¡ç®—PPOæŸå¤±
            policy_loss = self.ppo_loss(
                old_log_probs.detach(),
                new_log_probs,
                group_advantages.detach(),
                self.epsilon_clip
            )
            
            # KLæ•£åº¦æƒ©ç½š
            kl_div = self.compute_kl_divergence(generated_sequences, old_log_probs.detach())
            
            # æ€»æŸå¤±
            total_policy_loss = policy_loss + self.kl_coef * kl_div
            
            total_policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
            self.policy_optimizer.step()
            
            # æ›´æ–°ä»·å€¼ç½‘ç»œ
            self.value_optimizer.zero_grad()
            
            new_values = self.value_net(generated_sequences[:, :-1])
            value_loss = F.mse_loss(new_values, returns.detach())
            
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)
            self.value_optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            metrics['policy_loss'] += policy_loss.item()
            metrics['value_loss'] += value_loss.item()
            metrics['kl_div'] += kl_div.item()
        
        # å¹³å‡æŒ‡æ ‡
        for key in metrics:
            metrics[key] /= self.num_epochs
        
        return metrics
    
    def generate_and_train(self, prompts, max_length=20):
        """
        ç”Ÿæˆåºåˆ—å¹¶è®­ç»ƒ
        å‚æ•°:
            prompts: æç¤ºåºåˆ— [batch_size, prompt_len]
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        è¿”å›:
            generated: ç”Ÿæˆçš„åºåˆ—
            metrics: è®­ç»ƒæŒ‡æ ‡
        """
        # 1. ç”Ÿæˆåºåˆ—
        with torch.no_grad():
            generated, log_probs = self.policy.generate(prompts, max_length)
        
        # 2. è®¡ç®—å¥–åŠ±
        with torch.no_grad():
            rewards = self.reward_model(generated)
        
        # 3. è®­ç»ƒ
        metrics = self.train_step(prompts, generated, rewards)
        
        return generated, metrics


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 80)
    print("GRPO (Group Relative Policy Optimization) æµ‹è¯•")
    print("=" * 80)
    
    # è®¾ç½®å‚æ•°
    vocab_size = 1000
    embed_dim = 128
    hidden_dim = 256
    batch_size = 8
    prompt_len = 5
    max_gen_length = 10
    group_size = 4  # GRPOçš„ç»„å¤§å°
    
    # åˆ›å»ºç½‘ç»œ
    policy = PolicyNetwork(vocab_size, embed_dim, hidden_dim, num_layers=2)
    value_net = ValueNetwork(vocab_size, embed_dim, hidden_dim, num_layers=2)
    reward_model = RewardModel(vocab_size, embed_dim, hidden_dim)
    
    # åˆ›å»ºGRPOè®­ç»ƒå™¨
    grpo = GRPO(
        policy=policy,
        value_net=value_net,
        reward_model=reward_model,
        lr_policy=1e-4,
        lr_value=1e-4,
        gamma=0.99,
        lambda_gae=0.95,
        epsilon_clip=0.2,
        num_epochs=4,
        group_size=group_size,
        kl_coef=0.1
    )
    
    print("\n1. æ¨¡å‹å‚æ•°ç»Ÿè®¡")
    print("-" * 80)
    policy_params = sum(p.numel() for p in policy.parameters())
    value_params = sum(p.numel() for p in value_net.parameters())
    reward_params = sum(p.numel() for p in reward_model.parameters())
    
    print(f"ç­–ç•¥ç½‘ç»œå‚æ•°: {policy_params:,}")
    print(f"ä»·å€¼ç½‘ç»œå‚æ•°: {value_params:,}")
    print(f"å¥–åŠ±æ¨¡å‹å‚æ•°: {reward_params:,}")
    print(f"æ€»å‚æ•°: {policy_params + value_params + reward_params:,}")
    
    # åˆ›å»ºç¤ºä¾‹æç¤º
    prompts = torch.randint(0, vocab_size, (batch_size, prompt_len))
    
    print("\n2. ç”Ÿæˆæµ‹è¯•")
    print("-" * 80)
    with torch.no_grad():
        generated, log_probs = policy.generate(prompts, max_length=max_gen_length)
    
    print(f"æç¤ºå½¢çŠ¶: {prompts.shape}")
    print(f"ç”Ÿæˆåºåˆ—å½¢çŠ¶: {generated.shape}")
    print(f"å¯¹æ•°æ¦‚ç‡å½¢çŠ¶: {log_probs.shape}")
    
    print("\n3. å¥–åŠ±è®¡ç®—æµ‹è¯•")
    print("-" * 80)
    with torch.no_grad():
        rewards = reward_model(generated)
    
    print(f"å¥–åŠ±å½¢çŠ¶: {rewards.shape}")
    print(f"å¥–åŠ±ç»Ÿè®¡: å‡å€¼={rewards.mean().item():.4f}, æ ‡å‡†å·®={rewards.std().item():.4f}")
    
    print("\n4. GRPOè®­ç»ƒæµ‹è¯•")
    print("-" * 80)
    
    # æ‰§è¡Œä¸€æ¬¡è®­ç»ƒè¿­ä»£
    generated, metrics = grpo.generate_and_train(prompts, max_length=max_gen_length)
    
    print(f"ç­–ç•¥æŸå¤±: {metrics['policy_loss']:.4f}")
    print(f"ä»·å€¼æŸå¤±: {metrics['value_loss']:.4f}")
    print(f"KLæ•£åº¦: {metrics['kl_div']:.4f}")
    
    print("\n5. ç»„å†…ä¼˜åŠ¿è®¡ç®—æµ‹è¯•")
    print("-" * 80)
    
    # åˆ›å»ºæµ‹è¯•ä¼˜åŠ¿
    test_advantages = torch.randn(batch_size, max_gen_length)
    group_advantages = grpo.compute_group_advantages(test_advantages, group_size)
    
    print(f"åŸå§‹ä¼˜åŠ¿å½¢çŠ¶: {test_advantages.shape}")
    print(f"ç»„å†…ä¼˜åŠ¿å½¢çŠ¶: {group_advantages.shape}")
    
    # éªŒè¯ç»„å†…å½’ä¸€åŒ–
    num_groups = batch_size // group_size
    for g in range(num_groups):
        group_advs = group_advantages[g * group_size:(g + 1) * group_size]
        print(f"ç»„{g}: å‡å€¼={group_advs.mean().item():.4f}, æ ‡å‡†å·®={group_advs.std().item():.4f}")
    
    print("\n6. å®Œæ•´è®­ç»ƒå¾ªç¯ç¤ºä¾‹")
    print("-" * 80)
    print("""
    # GRPOè®­ç»ƒå¾ªç¯
    
    for iteration in range(num_iterations):
        # 1. é‡‡æ ·æç¤º
        prompts = sample_prompts(batch_size)
        
        # 2. ç”Ÿæˆå¹¶è®­ç»ƒ
        generated, metrics = grpo.generate_and_train(prompts, max_length=20)
        
        # 3. è®°å½•æŒ‡æ ‡
        print(f"Iter {iteration}: "
              f"Policy Loss={metrics['policy_loss']:.4f}, "
              f"Value Loss={metrics['value_loss']:.4f}, "
              f"KL={metrics['kl_div']:.4f}")
        
        # 4. å®šæœŸè¯„ä¼°
        if iteration % 100 == 0:
            evaluate_model(policy, test_prompts)
    """)
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ GRPO vs PPO çš„å…³é”®åŒºåˆ«")
    print("=" * 80)
    print("âœ… PPO: ä½¿ç”¨å…¨å±€ä¼˜åŠ¿ä¼°è®¡ï¼ˆæ‰€æœ‰æ ·æœ¬çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰")
    print("âœ… GRPO: ä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆæ¯ç»„å†…éƒ¨å½’ä¸€åŒ–ï¼‰")
    print("âœ… ä¼˜åŠ¿: å‡å°‘ä¸åŒæ‰¹æ¬¡ä¹‹é—´çš„æ–¹å·®ï¼Œè®­ç»ƒæ›´ç¨³å®š")
    print("âœ… é€‚ç”¨: ç‰¹åˆ«é€‚åˆè¯­è¨€æ¨¡å‹çš„RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰")
    print("=" * 80)
    
    print("\n" + "=" * 80)
    print("ğŸ“š GRPOåº”ç”¨åœºæ™¯")
    print("=" * 80)
    print("1. å¯¹è¯æ¨¡å‹ä¼˜åŒ–ï¼ˆChatGPTé£æ ¼ï¼‰")
    print("2. ä»£ç ç”Ÿæˆæ¨¡å‹å¾®è°ƒ")
    print("3. æ‘˜è¦ç”Ÿæˆä¼˜åŒ–")
    print("4. ä»»ä½•éœ€è¦RLHFçš„è¯­è¨€ç”Ÿæˆä»»åŠ¡")
    print("=" * 80)
