"""
å°†MoEé›†æˆåˆ°Transformerä¸­çš„å®Œæ•´ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨Encoder/Decoderä¸­ä½¿ç”¨æ··åˆä¸“å®¶æ¶æ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# ä»å·²æœ‰æ–‡ä»¶å¯¼å…¥
from Transformer import MultiHeadAttention, PositionalEncoding
from MoE import MoEEfficient, LoadBalancingLoss


class EncoderLayerWithMoE(nn.Module):
    """ä½¿ç”¨MoEçš„ç¼–ç å™¨å±‚"""
    
    def __init__(self, d_model, num_heads, d_ff, num_experts=8, top_k=2, dropout=0.1):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: FFNéšè—å±‚ç»´åº¦
            num_experts: MoEä¸“å®¶æ•°é‡
            top_k: æ¯æ¬¡æ¿€æ´»çš„ä¸“å®¶æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(EncoderLayerWithMoE, self).__init__()
        
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        # æ··åˆä¸“å®¶å±‚ï¼ˆæ›¿ä»£æ ‡å‡†FFNï¼‰
        self.moe = MoEEfficient(d_model, d_ff, num_experts, top_k, dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
            mask: æ©ç çŸ©é˜µ
        è¿”å›:
            x: è¾“å‡ºå¼ é‡
            aux_loss: è¾…åŠ©æŸå¤±ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
        """
        # å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + Layer Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # MoEå±‚ + æ®‹å·®è¿æ¥ + Layer Norm
        moe_output = self.moe(x)
        x = self.norm2(x + self.dropout(moe_output))
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        gates, indices = self.moe.router(x)
        lb_loss_fn = LoadBalancingLoss(self.moe.num_experts)
        aux_loss = lb_loss_fn(gates, indices)
        
        return x, aux_loss


class DecoderLayerWithMoE(nn.Module):
    """ä½¿ç”¨MoEçš„è§£ç å™¨å±‚"""
    
    def __init__(self, d_model, num_heads, d_ff, num_experts=8, top_k=2, dropout=0.1):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: FFNéšè—å±‚ç»´åº¦
            num_experts: MoEä¸“å®¶æ•°é‡
            top_k: æ¯æ¬¡æ¿€æ´»çš„ä¸“å®¶æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(DecoderLayerWithMoE, self).__init__()
        
        # æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        # è·¨æ³¨æ„åŠ›æœºåˆ¶
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        
        # æ··åˆä¸“å®¶å±‚ï¼ˆæ›¿ä»£æ ‡å‡†FFNï¼‰
        self.moe = MoEEfficient(d_model, d_ff, num_experts, top_k, dropout)
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è§£ç å™¨è¾“å…¥
            encoder_output: ç¼–ç å™¨è¾“å‡º
            src_mask: æºåºåˆ—æ©ç 
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç 
        è¿”å›:
            x: è¾“å‡ºå¼ é‡
            aux_loss: è¾…åŠ©æŸå¤±
        """
        # æ©ç è‡ªæ³¨æ„åŠ› + æ®‹å·® + LN
        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(self_attn_output))
        
        # è·¨æ³¨æ„åŠ› + æ®‹å·® + LN
        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # MoEå±‚ + æ®‹å·® + LN
        moe_output = self.moe(x)
        x = self.norm3(x + self.dropout(moe_output))
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        gates, indices = self.moe.router(x)
        lb_loss_fn = LoadBalancingLoss(self.moe.num_experts)
        aux_loss = lb_loss_fn(gates, indices)
        
        return x, aux_loss


class TransformerWithMoE(nn.Module):
    """ä½¿ç”¨MoEçš„å®Œæ•´Transformeræ¨¡å‹"""
    
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,
                 num_encoder_layers=2, num_decoder_layers=2, d_ff=2048,
                 num_experts=8, top_k=2, max_seq_length=5000, dropout=0.1,
                 aux_loss_weight=0.01):
        """
        å‚æ•°:
            src_vocab_size: æºè¯­è¨€è¯æ±‡è¡¨å¤§å°
            tgt_vocab_size: ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            num_encoder_layers: ç¼–ç å™¨å±‚æ•°
            num_decoder_layers: è§£ç å™¨å±‚æ•°
            d_ff: FFNç»´åº¦
            num_experts: MoEä¸“å®¶æ•°é‡
            top_k: æ¯æ¬¡æ¿€æ´»çš„ä¸“å®¶æ•°
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
            dropout: Dropoutæ¯”ç‡
            aux_loss_weight: è¾…åŠ©æŸå¤±æƒé‡
        """
        super(TransformerWithMoE, self).__init__()
        
        # è¯åµŒå…¥
        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)
        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        
        # ä½¿ç”¨MoEçš„ç¼–ç å™¨å±‚
        self.encoder_layers = nn.ModuleList([
            EncoderLayerWithMoE(d_model, num_heads, d_ff, num_experts, top_k, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        # ä½¿ç”¨MoEçš„è§£ç å™¨å±‚
        self.decoder_layers = nn.ModuleList([
            DecoderLayerWithMoE(d_model, num_heads, d_ff, num_experts, top_k, dropout)
            for _ in range(num_decoder_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
        self.aux_loss_weight = aux_loss_weight
        
    def generate_mask(self, src, tgt):
        """ç”Ÿæˆæ©ç """
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        
        tgt_seq_len = tgt.size(1)
        tgt_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=tgt.device)).bool()
        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1)
        
        return src_mask, tgt_mask
    
    def forward(self, src, tgt):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            src: æºåºåˆ—
            tgt: ç›®æ ‡åºåˆ—
        è¿”å›:
            output: è¾“å‡ºlogits
            aux_loss: æ€»çš„è¾…åŠ©æŸå¤±
        """
        # ç”Ÿæˆæ©ç 
        src_mask, tgt_mask = self.generate_mask(src, tgt)
        
        # ç¼–ç å™¨
        x = self.encoder_embedding(src) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        x = self.dropout(x)
        
        total_aux_loss = 0
        for encoder_layer in self.encoder_layers:
            x, aux_loss = encoder_layer(x, src_mask)
            total_aux_loss += aux_loss
        
        encoder_output = x
        
        # è§£ç å™¨
        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        x = self.dropout(x)
        
        for decoder_layer in self.decoder_layers:
            x, aux_loss = decoder_layer(x, encoder_output, src_mask, tgt_mask)
            total_aux_loss += aux_loss
        
        # è¾“å‡ºå±‚
        output = self.fc_out(x)
        
        # è¿”å›è¾“å‡ºå’Œè¾…åŠ©æŸå¤±
        return output, total_aux_loss * self.aux_loss_weight


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 80)
    print("MoE-Transformeré›†æˆæµ‹è¯•")
    print("=" * 80)
    
    # æ¨¡å‹å‚æ•°
    src_vocab_size = 5000
    tgt_vocab_size = 5000
    d_model = 512
    num_heads = 8
    num_encoder_layers = 2
    num_decoder_layers = 2
    d_ff = 2048
    num_experts = 8      # MoEä¸“å®¶æ•°
    top_k = 2            # æ¯æ¬¡æ¿€æ´»2ä¸ªä¸“å®¶
    
    # åˆ›å»ºæ ‡å‡†Transformerï¼ˆå¯¹æ¯”ï¼‰
    print("\n1. æ ‡å‡†Transformerå‚æ•°ç»Ÿè®¡")
    print("-" * 80)
    
    # ç®€å•è®¡ç®—ï¼šæ¯å±‚çš„FFNå‚æ•°
    standard_ffn_params = d_model * d_ff * 2  # W1 + W2
    standard_attention_params = 4 * d_model * d_model  # Q, K, V, O
    standard_layer_params = standard_ffn_params + standard_attention_params
    standard_total = (num_encoder_layers + num_decoder_layers) * standard_layer_params
    
    print(f"æ¯å±‚FFNå‚æ•°: {standard_ffn_params:,}")
    print(f"æ¯å±‚Attentionå‚æ•°: {standard_attention_params:,}")
    print(f"æ¯å±‚æ€»å‚æ•°: {standard_layer_params:,}")
    print(f"æ‰€æœ‰å±‚æ€»å‚æ•°: {standard_total:,}")
    
    # MoE-Transformerå‚æ•°
    print("\n2. MoE-Transformerå‚æ•°ç»Ÿè®¡")
    print("-" * 80)
    
    moe_ffn_params = num_experts * standard_ffn_params  # 8ä¸ªä¸“å®¶
    moe_router_params = d_model * num_experts  # è·¯ç”±ç½‘ç»œ
    moe_layer_params = moe_ffn_params + moe_router_params + standard_attention_params
    moe_total = (num_encoder_layers + num_decoder_layers) * moe_layer_params
    
    print(f"æ¯å±‚MoEå‚æ•°: {moe_ffn_params:,} ({num_experts}ä¸ªä¸“å®¶)")
    print(f"æ¯å±‚Routerå‚æ•°: {moe_router_params:,}")
    print(f"æ¯å±‚Attentionå‚æ•°: {standard_attention_params:,}")
    print(f"æ¯å±‚æ€»å‚æ•°: {moe_layer_params:,}")
    print(f"æ‰€æœ‰å±‚æ€»å‚æ•°: {moe_total:,}")
    
    # æ•ˆç‡å¯¹æ¯”
    print("\n3. æ•ˆç‡å¯¹æ¯”")
    print("-" * 80)
    param_increase = moe_total / standard_total
    compute_increase = top_k / num_experts * param_increase
    
    print(f"å‚æ•°é‡å¢åŠ : {param_increase:.1f}x")
    print(f"è®¡ç®—é‡å¢åŠ : {compute_increase:.1f}x")
    print(f"æ•ˆç‡æå‡: {param_increase / compute_increase:.1f}x")
    print(f"(æ‹¥æœ‰{param_increase:.1f}xçš„å‚æ•°ï¼Œä½†åªéœ€è¦{compute_increase:.1f}xçš„è®¡ç®—)")
    
    # åˆ›å»ºå®é™…æ¨¡å‹
    print("\n4. å®é™…æ¨¡å‹æµ‹è¯•")
    print("-" * 80)
    
    model = TransformerWithMoE(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=d_model,
        num_heads=num_heads,
        num_encoder_layers=num_encoder_layers,
        num_decoder_layers=num_decoder_layers,
        d_ff=d_ff,
        num_experts=num_experts,
        top_k=top_k,
        aux_loss_weight=0.01
    )
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    batch_size = 2
    src_seq_len = 10
    tgt_seq_len = 8
    
    src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))
    tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output, aux_loss = model(src, tgt)
    
    print(f"æºåºåˆ—å½¢çŠ¶: {src.shape}")
    print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾…åŠ©æŸå¤±: {aux_loss.item():.6f}")
    print(f"æ¨¡å‹æ€»å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒç¤ºä¾‹
    print("\n5. è®­ç»ƒæµç¨‹ç¤ºä¾‹")
    print("-" * 80)
    print("""
    # åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨MoE-Transformer
    
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    for batch in dataloader:
        src, tgt = batch
        
        # å‰å‘ä¼ æ’­
        output, aux_loss = model(src, tgt[:, :-1])  # teacher forcing
        
        # è®¡ç®—ä¸»ä»»åŠ¡æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        main_loss = F.cross_entropy(
            output.reshape(-1, vocab_size),
            tgt[:, 1:].reshape(-1)
        )
        
        # æ€»æŸå¤± = ä¸»æŸå¤± + è¾…åŠ©æŸå¤±ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
        total_loss = main_loss + aux_loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        print(f'Main Loss: {main_loss.item():.4f}, Aux Loss: {aux_loss.item():.4f}')
    """)
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ MoEåœ¨Transformerä¸­çš„ä¼˜åŠ¿")
    print("=" * 80)
    print(f"âœ… å‚æ•°å®¹é‡æå‡ {num_experts}xï¼Œä½†æ¨ç†æ—¶ä»…æ¿€æ´» {top_k}/{num_experts} çš„ä¸“å®¶")
    print(f"âœ… æ¯ä¸ªtokenå¯ä»¥è¢«ä¸“é—¨çš„ä¸“å®¶å¤„ç†ï¼Œæå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›")
    print(f"âœ… è®¡ç®—é‡ä»…å¢åŠ  {top_k}xï¼Œè¿œå°äºå‚æ•°å¢åŠ çš„ {num_experts}x")
    print(f"âœ… ç‰¹åˆ«é€‚åˆè¶…å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚GPT-4ã€Switch Transformerç­‰ï¼‰")
    print("=" * 80)
