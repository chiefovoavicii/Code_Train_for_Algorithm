"""
æ··åˆä¸“å®¶æ¨¡å‹(Mixture of Experts, MoE)æœ€å°å®ç°
ç”¨äºæ›¿ä»£Transformerä¸­çš„FFNå±‚ï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class Expert(nn.Module):
    """å•ä¸ªä¸“å®¶ç½‘ç»œï¼ˆç­‰ä»·äºæ ‡å‡†FFNï¼‰"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        å‚æ•°:
            d_model: è¾“å…¥/è¾“å‡ºç»´åº¦
            d_ff: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¯”ç‡
        """
        super(Expert, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: [batch_size, seq_len, d_model] æˆ– [num_tokens, d_model]
        """
        # FFN: W2(Dropout(ReLU(W1(x))))
        x = self.linear1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x


class Router(nn.Module):
    """è·¯ç”±ç½‘ç»œï¼ˆé—¨æ§ç½‘ç»œï¼‰- å†³å®šæ¯ä¸ªtokenç”±å“ªäº›ä¸“å®¶å¤„ç†"""
    
    def __init__(self, d_model, num_experts, top_k=2):
        """
        å‚æ•°:
            d_model: è¾“å…¥ç»´åº¦
            num_experts: ä¸“å®¶æ€»æ•°
            top_k: æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        """
        super(Router, self).__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        # é—¨æ§ç½‘ç»œï¼šå°†è¾“å…¥æ˜ å°„åˆ°ä¸“å®¶åˆ†æ•°
        self.gate = nn.Linear(d_model, num_experts)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: [batch_size, seq_len, d_model]
        è¿”å›:
            gates: ä¸“å®¶æƒé‡ [batch_size, seq_len, top_k]
            indices: é€‰ä¸­çš„ä¸“å®¶ç´¢å¼• [batch_size, seq_len, top_k]
        """
        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„logits
        logits = self.gate(x)  # [batch_size, seq_len, num_experts]
        
        # é€‰æ‹©top-kä¸ªä¸“å®¶
        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)
        
        # å¯¹é€‰ä¸­çš„ä¸“å®¶åšsoftmaxå½’ä¸€åŒ–
        top_k_gates = F.softmax(top_k_logits, dim=-1)
        
        return top_k_gates, top_k_indices


class MixtureOfExperts(nn.Module):
    """æ··åˆä¸“å®¶å±‚ï¼ˆMoE Layerï¼‰- æ›¿ä»£æ ‡å‡†FFN"""
    
    def __init__(self, d_model, d_ff, num_experts=8, top_k=2, dropout=0.1):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦
            d_ff: ä¸“å®¶ç½‘ç»œéšè—å±‚ç»´åº¦
            num_experts: ä¸“å®¶æ€»æ•°
            top_k: æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼ˆé€šå¸¸ä¸º2ï¼‰
            dropout: Dropoutæ¯”ç‡
        """
        super(MixtureOfExperts, self).__init__()
        
        self.num_experts = num_experts
        self.top_k = top_k
        self.d_model = d_model
        
        # åˆ›å»ºå¤šä¸ªä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff, dropout) 
            for _ in range(num_experts)
        ])
        
        # è·¯ç”±ç½‘ç»œ
        self.router = Router(d_model, num_experts, top_k)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        å‚æ•°:
            x: [batch_size, seq_len, d_model]
        è¿”å›:
            output: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.shape
        
        # 1. è·¯ç”±ï¼šé€‰æ‹©ä¸“å®¶
        gates, indices = self.router(x)  # gates: [B, S, K], indices: [B, S, K]
        
        # 2. åˆå§‹åŒ–è¾“å‡º
        output = torch.zeros_like(x)  # [batch_size, seq_len, d_model]
        
        # 3. å¯¹æ¯ä¸ªé€‰ä¸­çš„ä¸“å®¶è¿›è¡Œè®¡ç®—
        # æ–¹æ³•1ï¼šå¾ªç¯æ–¹å¼ï¼ˆç®€å•ä½†æ•ˆç‡è¾ƒä½ï¼‰
        for i in range(self.top_k):
            # è·å–ç¬¬iä¸ªä¸“å®¶çš„ç´¢å¼•å’Œæƒé‡
            expert_indices = indices[:, :, i]  # [batch_size, seq_len]
            expert_gates = gates[:, :, i].unsqueeze(-1)  # [batch_size, seq_len, 1]
            
            # å¯¹æ¯ä¸ªä¸“å®¶IDè¿›è¡Œå¤„ç†
            for expert_id in range(self.num_experts):
                # æ‰¾åˆ°ä½¿ç”¨å½“å‰ä¸“å®¶çš„token
                mask = (expert_indices == expert_id)  # [batch_size, seq_len]
                
                if mask.any():
                    # æå–éœ€è¦è¯¥ä¸“å®¶å¤„ç†çš„token
                    expert_input = x[mask]  # [num_tokens, d_model]
                    
                    # ä¸“å®¶å¤„ç†
                    expert_output = self.experts[expert_id](expert_input)  # [num_tokens, d_model]
                    
                    # å°†ç»“æœåŠ æƒååŠ å›åˆ°è¾“å‡º
                    # æ³¨æ„ï¼šéœ€è¦å°†maskæ‰©å±•åˆ°d_modelç»´åº¦
                    mask_expanded = mask.unsqueeze(-1).expand_as(x)  # [batch_size, seq_len, d_model]
                    gate_expanded = expert_gates.expand_as(x)  # [batch_size, seq_len, d_model]
                    
                    # åˆ›å»ºä¸´æ—¶è¾“å‡ºå¼ é‡
                    temp_output = torch.zeros_like(x)
                    temp_output[mask] = expert_output
                    
                    # åŠ æƒç´¯åŠ 
                    output = output + temp_output * gate_expanded * mask_expanded.float()
        
        return output


class MoEEfficient(nn.Module):
    """é«˜æ•ˆçš„æ··åˆä¸“å®¶å±‚å®ç°ï¼ˆä½¿ç”¨batchå¤„ç†ï¼‰"""
    
    def __init__(self, d_model, d_ff, num_experts=8, top_k=2, dropout=0.1):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦
            d_ff: ä¸“å®¶ç½‘ç»œéšè—å±‚ç»´åº¦
            num_experts: ä¸“å®¶æ€»æ•°
            top_k: æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•°é‡
            dropout: Dropoutæ¯”ç‡
        """
        super(MoEEfficient, self).__init__()
        
        self.num_experts = num_experts
        self.top_k = top_k
        self.d_model = d_model
        
        # åˆ›å»ºä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff, dropout) 
            for _ in range(num_experts)
        ])
        
        # è·¯ç”±ç½‘ç»œ
        self.router = Router(d_model, num_experts, top_k)
        
    def forward(self, x):
        """
        é«˜æ•ˆçš„å‰å‘ä¼ æ’­å®ç°
        å‚æ•°:
            x: [batch_size, seq_len, d_model]
        """
        original_shape = x.shape
        batch_size, seq_len, d_model = x.shape
        
        # 1. å±•å¹³è¾“å…¥ä¾¿äºå¤„ç†
        x_flat = x.view(-1, d_model)  # [batch_size * seq_len, d_model]
        
        # 2. è·¯ç”±
        gates, indices = self.router(x)  # [B, S, K]
        gates_flat = gates.view(-1, self.top_k)  # [B*S, K]
        indices_flat = indices.view(-1, self.top_k)  # [B*S, K]
        
        # 3. åˆå§‹åŒ–è¾“å‡º
        output_flat = torch.zeros_like(x_flat)  # [B*S, d_model]
        
        # 4. ä¸ºæ¯ä¸ªä¸“å®¶æ‰¹é‡å¤„ç†
        for expert_id in range(self.num_experts):
            # æ‰¾åˆ°æ‰€æœ‰ä½¿ç”¨è¯¥ä¸“å®¶çš„ä½ç½®
            expert_mask = (indices_flat == expert_id)  # [B*S, K]
            
            # è·å–ä½¿ç”¨è¯¥ä¸“å®¶çš„tokenç´¢å¼•
            token_indices, k_indices = torch.where(expert_mask)
            
            if len(token_indices) > 0:
                # æå–è¾“å…¥
                expert_input = x_flat[token_indices]  # [num_tokens, d_model]
                
                # ä¸“å®¶å¤„ç†
                expert_output = self.experts[expert_id](expert_input)  # [num_tokens, d_model]
                
                # è·å–å¯¹åº”çš„é—¨æ§æƒé‡
                expert_gates = gates_flat[token_indices, k_indices].unsqueeze(-1)  # [num_tokens, 1]
                
                # åŠ æƒç´¯åŠ åˆ°è¾“å‡º
                output_flat[token_indices] += expert_output * expert_gates
        
        # 5. æ¢å¤åŸå§‹å½¢çŠ¶
        output = output_flat.view(original_shape)
        
        return output


class LoadBalancingLoss(nn.Module):
    """è´Ÿè½½å‡è¡¡æŸå¤± - é¼“åŠ±ä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨"""
    
    def __init__(self, num_experts):
        """
        å‚æ•°:
            num_experts: ä¸“å®¶æ€»æ•°
        """
        super(LoadBalancingLoss, self).__init__()
        self.num_experts = num_experts
        
    def forward(self, gates, indices):
        """
        è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        å‚æ•°:
            gates: é—¨æ§æƒé‡ [batch_size, seq_len, top_k]
            indices: ä¸“å®¶ç´¢å¼• [batch_size, seq_len, top_k]
        è¿”å›:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
        expert_counts = torch.zeros(self.num_experts, device=gates.device)
        expert_gates_sum = torch.zeros(self.num_experts, device=gates.device)
        
        batch_size, seq_len, top_k = gates.shape
        total_tokens = batch_size * seq_len
        
        # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨æƒ…å†µ
        for i in range(self.num_experts):
            mask = (indices == i)  # [batch_size, seq_len, top_k]
            expert_counts[i] = mask.sum().float()
            expert_gates_sum[i] = (gates * mask.float()).sum()
        
        # è®¡ç®—é¢‘ç‡å’Œæƒé‡çš„å‡å€¼
        freq = expert_counts / (total_tokens * top_k)  # é€‰ä¸­é¢‘ç‡
        gate_mean = expert_gates_sum / (total_tokens * top_k)  # å¹³å‡æƒé‡
        
        # è´Ÿè½½å‡è¡¡æŸå¤±ï¼šfreq * gate_mean çš„æ–¹å·®è¶Šå°è¶Šå¥½
        loss = self.num_experts * (freq * gate_mean).sum()
        
        return loss


# ä½¿ç”¨ç¤ºä¾‹å’Œå¯¹æ¯”
if __name__ == "__main__":
    print("=" * 80)
    print("æ··åˆä¸“å®¶æ¨¡å‹(MoE)æµ‹è¯•")
    print("=" * 80)
    
    # è®¾ç½®å‚æ•°
    batch_size = 2
    seq_len = 10
    d_model = 512
    d_ff = 2048
    num_experts = 8
    top_k = 2
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. æ ‡å‡†FFN
    print("\n1. æ ‡å‡†FFN")
    print("-" * 80)
    standard_ffn = Expert(d_model, d_ff)
    standard_params = sum(p.numel() for p in standard_ffn.parameters())
    print(f"å‚æ•°é‡: {standard_params:,}")
    
    with torch.no_grad():
        output_standard = standard_ffn(x)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output_standard.shape}")
    
    # 2. åŸºç¡€MoEå®ç°
    print("\n2. åŸºç¡€MoEå®ç°")
    print("-" * 80)
    moe = MixtureOfExperts(d_model, d_ff, num_experts, top_k)
    moe_params = sum(p.numel() for p in moe.parameters())
    print(f"ä¸“å®¶æ€»æ•°: {num_experts}")
    print(f"æ¯æ¬¡æ¿€æ´»ä¸“å®¶æ•°: {top_k}")
    print(f"æ€»å‚æ•°é‡: {moe_params:,}")
    print(f"å®é™…è®¡ç®—å‚æ•°é‡: {moe_params / num_experts * top_k:,.0f} (çº¦ä¸ºæ ‡å‡†FFNçš„ {top_k/num_experts*100:.1f}%)")
    
    with torch.no_grad():
        output_moe = moe(x)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output_moe.shape}")
    
    # 3. é«˜æ•ˆMoEå®ç°
    print("\n3. é«˜æ•ˆMoEå®ç°")
    print("-" * 80)
    moe_efficient = MoEEfficient(d_model, d_ff, num_experts, top_k)
    
    with torch.no_grad():
        output_moe_efficient = moe_efficient(x)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output_moe_efficient.shape}")
    
    # 4. è´Ÿè½½å‡è¡¡æŸå¤±
    print("\n4. è´Ÿè½½å‡è¡¡æŸå¤±æµ‹è¯•")
    print("-" * 80)
    lb_loss = LoadBalancingLoss(num_experts)
    
    with torch.no_grad():
        gates, indices = moe.router(x)
        loss_value = lb_loss(gates, indices)
    
    print(f"é—¨æ§æƒé‡å½¢çŠ¶: {gates.shape}")
    print(f"ä¸“å®¶ç´¢å¼•å½¢çŠ¶: {indices.shape}")
    print(f"è´Ÿè½½å‡è¡¡æŸå¤±: {loss_value.item():.4f}")
    
    # 5. å‚æ•°å¯¹æ¯”
    print("\n5. å‚æ•°æ•ˆç‡å¯¹æ¯”")
    print("-" * 80)
    print(f"{'æ¨¡å‹':<20} {'æ€»å‚æ•°é‡':<15} {'è®¡ç®—å‚æ•°é‡':<15} {'æ•ˆç‡æå‡'}")
    print("-" * 80)
    print(f"{'æ ‡å‡†FFN':<20} {standard_params:>12,}  {standard_params:>12,}  {'1.0x'}")
    actual_compute = moe_params / num_experts * top_k
    efficiency = standard_params / actual_compute
    print(f"{'MoE (8ä¸“å®¶, top2)':<20} {moe_params:>12,}  {actual_compute:>12,.0f}  {efficiency:.1f}x")
    
    # 6. è·¯ç”±åˆ†æ
    print("\n6. è·¯ç”±åˆ†æï¼ˆæŸ¥çœ‹ä¸“å®¶é€‰æ‹©åˆ†å¸ƒï¼‰")
    print("-" * 80)
    with torch.no_grad():
        gates, indices = moe.router(x)
        
        # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
        expert_usage = torch.zeros(num_experts)
        for i in range(num_experts):
            expert_usage[i] = (indices == i).sum().item()
        
        total_selections = expert_usage.sum().item()
        print(f"æ€»é€‰æ‹©æ¬¡æ•°: {total_selections:.0f}")
        print(f"\nä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ:")
        for i in range(num_experts):
            usage_pct = expert_usage[i] / total_selections * 100
            bar = 'â–ˆ' * int(usage_pct / 2)
            print(f"  ä¸“å®¶ {i}: {expert_usage[i]:>3.0f} æ¬¡ ({usage_pct:>5.1f}%) {bar}")
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    
    # 7. æ€§èƒ½ä¼˜åŠ¿æ€»ç»“
    print("\nğŸ’¡ MoEä¼˜åŠ¿æ€»ç»“:")
    print("-" * 80)
    print(f"âœ… æ¨¡å‹å®¹é‡æå‡: {num_experts}x (æ‹¥æœ‰{num_experts}ä¸ªä¸“å®¶)")
    print(f"âœ… è®¡ç®—é‡é™ä½: {num_experts/top_k:.1f}x (æ¯æ¬¡åªç”¨{top_k}ä¸ªä¸“å®¶)")
    print(f"âœ… å‚æ•°æ•ˆç‡: åœ¨å‚æ•°é‡å¢åŠ {num_experts}å€çš„æƒ…å†µä¸‹,è®¡ç®—é‡ä»…å¢åŠ {top_k}å€")
    print(f"âœ… ä¸“ä¸šåŒ–èƒ½åŠ›: ä¸åŒä¸“å®¶å¯ä»¥å­¦ä¹ å¤„ç†ä¸åŒç±»å‹çš„token")
    print("=" * 80)
