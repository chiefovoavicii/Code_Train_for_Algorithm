{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer编码器层核心代码（PyTorch示例）\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # 自注意力计算\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
    "        src = src + self.norm1(src2)  # 残差连接+归一化\n",
    "        # 前馈网络\n",
    "        src2 = self.linear2(F.relu(self.linear1(src)))\n",
    "        src = src + self.norm2(src2)\n",
    "        return \n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = k.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "    if mask: scaled_attention_logits += mask * -1e9\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "    return torch.matmul(attention_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self_attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads=1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size #输入向量的维度\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3 * embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.shape\n",
    "        #生成Q,K,V 通过一次线性变换分割成三部分\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)  #分割为Q/K/V\n",
    "\n",
    "        #计算缩放点积注意力\n",
    "        scale = (self.head_dim) ** -0.5  #缩放因子根号d， 防止点积过大\n",
    "        attention_scores =  torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "        #应用softmax得到注意力权重\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return out, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2631102471.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    def forward(self, x):\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class multiattention(nn.Module):\n",
    "    def __init__(self, embed_size,head=8):\n",
    "        self.head = head\n",
    "        self.embed_size = embed_size\n",
    "        self.head_dim = embed_size // head\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3*embed_size)\n",
    "        self.to_out = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self,x):\n",
    "        batch_size, seq_len, embed_size = x.shape()\n",
    "        qkv = self.to_qkv(x).view(batch_size, seq_len, 3, self.head, self.head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4)\n",
    "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attention_scores = torch.matmul(q,k.transpose(-2,-1)) * self.head_dim ** -0.5\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention_weights, v)\n",
    "\n",
    "        out = out.permute(0,2,1,3).contiguous.view(batch_size, seq_len, embed_size)\n",
    "\n",
    "        out = self.to_out(out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3180739669.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    assert embed_size % heads == 0, #embed_size 必须能被 heads 整除\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#MultiAttention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        assert embed_size % heads == 0, #embed_size 必须能被 heads 整除\n",
    "        self.heads_dim = embed_size // heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3 * embed_size)\n",
    "        self.to_out = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.shape\n",
    "\n",
    "        qkv = self.to_qkv(x).view(batch_size, seq_len, 3, self.heads, self.heads_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # 调整维度顺序为 (3, batch_size, heads, seq_len, heads_dim)  #方便之后K矩阵的转置\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # 分离为查询 q、键 k、值 v，每个形状为 (batch_size, heads, seq_len, heads_dim)\n",
    "        #q：batch_size, heads, seq_len, heads_dim\n",
    "        scale = self.heads_dim ** -0.5  # 计算缩放因子，防止点积过大，等于 1/sqrt(heads_dim)\n",
    "        attention_scores = torch.matmul(q,k.transpose(-2, -1)) * scale\n",
    "        # 计算注意力分数：q 和 k 的转置点积后乘以缩放因子\n",
    "        # k.transpose(-2, -1) 将 k 的最后两个维度 (seq_len, heads_dim) 转置为 (heads_dim, seq_len)\n",
    "        # 结果形状为 (batch_size, heads, seq_len, seq_len)\n",
    "\n",
    "        attention_wights = F.softmax(attention_scores, dim=-1)  # 对注意力分数应用 softmax，得到权重\n",
    "        # dim=-1 表示沿最后一个维度（序列长度）归一化，形状保持 (batch_size, heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attention_wights, v)\n",
    "        # attention_weights 和 v 的矩阵乘法，结果形状为 (batch_size, heads, seq_len, heads_dim)\n",
    "        print(out)\n",
    "        #用于合并最后两个维度heads*heads_dim\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, embed_size)\n",
    "        #permute后，batch_size, seq_len, heads, heads_dim\n",
    "        # contiguous() 确保内存连续，view 重塑为 (batch_size, seq_len, embed_size)\n",
    "        print(out)\n",
    "        #batch_size, seq_len, embed_size\n",
    "        out = self.to_out(out)\n",
    "        print(out)\n",
    "        return out   \n",
    "    \n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, num_head, groups, k_v_heads, embed_size):\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_size // num_head\n",
    "        self.num_head = num_head\n",
    "        self.groups = groups\n",
    "        self.k_v_heads = k_v_heads\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
    "        self.k_proj = nn.Linear(embed_size, k_v_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_size, k_v_heads * self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, head_dim = x.shape\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.groups, self.num_head, head_dim)\n",
    "        q = q.permute(0,2,3,1,4)\n",
    "        k = k.view(batch_size, seq_len, self.num_head, head_dim)\n",
    "        k = k.permute(0,2,3,1)\n",
    "        v = v.view(batch_size, seq_len, self.num_head, head_dim)\n",
    "        v = v.permute(0,2,1,3)\n",
    "\n",
    "        scale = self.head_dim ** -0.5\n",
    "        score = torch.matmul(q,k) * scale\n",
    "\n",
    "        weight = F.softmax(score, dim=-1)\n",
    "\n",
    "        out = torch.matmul(weight, v)\n",
    "\n",
    "        out = out.permute(0, 3, 1, 2, 4)\n",
    "        \n",
    "        out = out.contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "\n",
    "# 输入形状: (batch_size, seq_len, embed_size)\n",
    "# BN统计量在 (batch_size, seq_len) 上计算，对每个 embed_size 通道独立归一化\n",
    "bn = nn.BatchNorm1d(embed_size)\n",
    "x_bn = bn(x)  # 输出形状仍为 (batch_size, seq_len, embed_size)\n",
    "\n",
    "# 输入形状: (batch_size, seq_len, embed_size)\n",
    "# LN统计量在 embed_size 上计算，每个样本的每个位置独立归一化\n",
    "ln = nn.LayerNorm(embed_size)\n",
    "x_ln = ln(x)  # 输出形状仍为 (batch_size, seq_len, embed_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim = 1):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, lr=0.01):\n",
    "        # 手动初始化权重和偏置（对应线性层参数）\n",
    "        self.W = torch.randn(input_dim, 1, requires_grad=False) * 0.01\n",
    "        self.b = torch.zeros(1, requires_grad=False)\n",
    "        self.lr = lr  # 学习率\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播：计算线性输出并应用Sigmoid\"\"\"\n",
    "        linear = torch.mm(X, self.W) + self.b  # X.shape: (n_samples, input_dim)\n",
    "        return torch.sigmoid(linear)  # 输出概率 P(y=1|x)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"计算二元交叉熵损失\"\"\"\n",
    "        epsilon = 1e-8  # 防止log(0)\n",
    "        return -torch.mean(y_true * torch.log(y_pred + epsilon) + \n",
    "                          (1 - y_true) * torch.log(1 - y_pred + epsilon))\n",
    "\n",
    "    def manual_update(self, X, y):\n",
    "        \"\"\"手动计算梯度并更新参数（不依赖optimizer）\"\"\"\n",
    "        # 前向传播\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        \n",
    "        # 反向传播手动求导（链式法则）\n",
    "        n_samples = X.size(0)\n",
    "        \n",
    "        # 计算损失对输出的梯度 dL/dy_pred\n",
    "        dL_dy_pred = (y_pred - y) / n_samples  # shape: (n_samples, 1)\n",
    "        \n",
    "        # 计算梯度 dL/dW 和 dL/db\n",
    "        dL_dW = torch.mm(X.t(), dL_dy_pred)    # X.T @ dL_dy_pred\n",
    "        dL_db = torch.sum(dL_dy_pred, dim=0)   # 沿样本维度求和\n",
    "        \n",
    "        # 参数更新（梯度下降）\n",
    "        self.W -= self.lr * dL_dW\n",
    "        self.b -= self.lr * dL_db\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"预测类别（0或1）\"\"\"\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            proba = self.forward(X)\n",
    "            return (proba >= threshold).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2445662913.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    if pos_num==0 or neg_sum==0:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def auc_count(self,y_true,y_prob):\n",
    "    sorted_nums = torch.argsort(y_prob, descending=False)\n",
    "    y_prob = y_prob[sorted_nums]\n",
    "    y_true = y_true[sorted_nums]\n",
    "\n",
    "    pos_num = (y_true == 1).sum().item()\n",
    "    neg_num = (y_true == 0).sum().item()\n",
    "    \n",
    "    cnt_cum = 0\n",
    "    same_prob = None\n",
    "    same_prob_cnt = 0\n",
    "\n",
    "    for i, prob in enumerate(y_prob, start=1):\n",
    "        if prob != same_prob:\n",
    "            if same_prob_cnt > 0:\n",
    "                avg_prob = same_prob_cnt / same_prob_cnt\n",
    "                cnt_cum += avg_prob * same_prob_cnt\n",
    "\n",
    "                same_prob_cnt = 0\n",
    "                prob_sum = 0\n",
    "            same_prob = prob\n",
    "        if y_true[i-1] == 1:\n",
    "            same_prob_cnt += 1 \n",
    "            prob_sum += prob\n",
    "        \n",
    "    if same_prob_cnt>0:\n",
    "        avg_prob = prob_sum / same_prob_cnt\n",
    "        cnt_cum += avg_prob * same_prob_cnt\n",
    "\n",
    "    auc = (cnt_cum - pos_num*(pos_num+1)/2)/(pos_num * neg_num)\n",
    "\n",
    "    return auc \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_probs = y_prob[y_true == 1]\n",
    "neg_probs = y_prob[y_true == 0]\n",
    "comparison = pos_probs[:, None] > neg_probs[None, :]\n",
    "auc = (comparison.sum() + 0.5 * (pos_probs[:, None] == neg_probs[None, :]).sum()) / (pos_num * neg_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC计算代码 表示正样本预测概率大于负样本预测概率的概率，评估二分类模型对正负样本的排序能力，反映区分两类样本的效果\n",
    "\n",
    "# 按预测概率升序排列，并同步调整真实标签顺序\n",
    "def auc_count(self, y_prob, y_true):\n",
    "    sorted_nums = torch.argsort(y_prob, descending=False)\n",
    "    y_true_sorted = y_true[sorted_nums]\n",
    "    y_prob_sorted = y_prob[sorted_nums]\n",
    "\n",
    "    pos_num = (y_true == 1).sum().item()\n",
    "    neg_num = (y_true == 0).sum().item()\n",
    "\n",
    "    if pos_num == 0 or neg_num == 0:\n",
    "        return 0.0\n",
    "    # 计算每个正样本的累计排名（处理相同概率的情况）\n",
    "    cum_rank = 0.0\n",
    "    cur_prob = None\n",
    "    same_prob_count = 0\n",
    "    same_rank_sum = 0\n",
    "\n",
    "    for idx, prob in enumerate(y_prob_sorted, start = 1):\n",
    "        if prob != cur_prob:\n",
    "            if same_prob_count > 0:\n",
    "                avg_rank = same_rank_sum / same_prob_count\n",
    "                cum_rank += avg_rank * same_prob_count\n",
    "                \n",
    "                same_prob_count = 0\n",
    "                same_rank_sum = 0.0\n",
    "            cur_prob = prob\n",
    "        #只找正样本的排名\n",
    "        if y_true_sorted[idx - 1] == 1:  \n",
    "            same_prob_count += 1\n",
    "            same_rank_sum += idx\n",
    "    # 处理最后一组相同概率的情况\n",
    "    if same_prob_count > 0:\n",
    "        avg_rank = same_rank_sum/ same_prob_count\n",
    "        cum_rank += avg_rank * same_prob_count\n",
    "    # 应用AUC计算公式\n",
    "    auc = (cum_rank - pos_num*(pos_num+1)/2)/(pos_num * neg_num)  \n",
    "    return auc      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(self, P, Q, epsilon = 1e-8):\n",
    "    P += epsilon\n",
    "    Q += epsilon\n",
    "\n",
    "    P = F.softmax(P,dim=-1)\n",
    "    Q = F.softmax(Q,dim=-1)\n",
    "\n",
    "    logP = torch.log(P)\n",
    "    logQ = torch.log(Q)\n",
    "\n",
    "    KL_elements = P *(logP - logQ)\n",
    "    P/Q - logp - logQ - 1\n",
    "    KL = KL_elements.sum(dim=-1)\n",
    "\n",
    "    return KL.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: tensor([[0.0469, 0.0489, 0.2302, 0.0418, 0.0568, 0.0148, 0.0487, 0.2712, 0.2014,\n",
      "         0.0392],\n",
      "        [0.0436, 0.3497, 0.0228, 0.0051, 0.0119, 0.0270, 0.0196, 0.0320, 0.4057,\n",
      "         0.0826],\n",
      "        [0.1846, 0.1609, 0.1473, 0.0572, 0.2800, 0.0317, 0.0273, 0.0310, 0.0408,\n",
      "         0.0393],\n",
      "        [0.0768, 0.0925, 0.2772, 0.1169, 0.0447, 0.0567, 0.1478, 0.0679, 0.1017,\n",
      "         0.0178]])\n",
      "P1: tensor([[0.0469, 0.0489, 0.2302, 0.0418, 0.0568, 0.0148, 0.0487, 0.2712, 0.2014,\n",
      "         0.0392],\n",
      "        [0.0436, 0.3497, 0.0228, 0.0051, 0.0119, 0.0270, 0.0196, 0.0320, 0.4057,\n",
      "         0.0826],\n",
      "        [0.1846, 0.1609, 0.1473, 0.0572, 0.2800, 0.0317, 0.0273, 0.0310, 0.0408,\n",
      "         0.0393],\n",
      "        [0.0768, 0.0925, 0.2772, 0.1169, 0.0447, 0.0567, 0.1478, 0.0679, 0.1017,\n",
      "         0.0178]])\n",
      "tensor(1.0280)\n"
     ]
    }
   ],
   "source": [
    "def kl(P,Q,epsilon = 1e-8):\n",
    "    P += epsilon\n",
    "    Q += epsilon\n",
    "\n",
    "    P = F.softmax(P,dim=-1)\n",
    "    Q = F.softmax(Q, dim=-1)\n",
    "\n",
    "    logP = torch.log(P)\n",
    "    logQ = torch.log(Q)\n",
    "\n",
    "    KL = P * (logP - logQ)\n",
    "    KL = KL.sum(dim=-1)\n",
    "\n",
    "    return KL.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KL散度\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kl_divergence_manual(P, Q, epsilon=1e-8):\n",
    "    \"\"\"手动实现KL散度计算\"\"\"\n",
    "    # 添加极小值避免log(0)导致数值不稳定\n",
    "    P = P + epsilon\n",
    "    Q = Q + epsilon\n",
    "    \n",
    "    # 归一化处理（若输入不是概率分布）\n",
    "    # 沿最后一个维度求和并保持维度，将张量转化为概率分布\n",
    "    P = P / P.sum(dim=-1, keepdim=True)\n",
    "    Q = Q / Q.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # 计算对数概率（KL散度核心操作）\n",
    "    logP = torch.log(P)\n",
    "    logQ = torch.log(Q)\n",
    "    \n",
    "    # 逐元素计算KL散度：P * (logP - logQ)\n",
    "    kl_elements = P * (logP - logQ)\n",
    "    \n",
    "    # 沿最后一个维度求和（对每个样本的概率分布求和）\n",
    "    kl = kl_elements.sum(dim=-1)\n",
    "    \n",
    "    # 返回批次平均值\n",
    "    return kl.mean()\n",
    "\n",
    "def kl_divergence_pytorch(P_logits, Q_logits):\n",
    "    \"\"\"使用PyTorch内置函数计算KL散度\"\"\"\n",
    "    # 对Q的logits应用log_softmax得到对数概率（符合KLDivLoss输入要求）\n",
    "    log_Q = F.log_softmax(Q_logits, dim=-1)\n",
    "    \n",
    "    # 对P的logits应用softmax得到概率分布（符合KLDivLoss目标要求）\n",
    "    P = F.softmax(P_logits, dim=-1)\n",
    "    \n",
    "    # 初始化KL散度损失函数，reduction='batchmean'表示按批次均值返回\n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    # 计算KL散度（输入log_Q需在前，目标P在后）\n",
    "    loss = kl_loss(log_Q, P)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT模型核心结构（简化版）\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.segment_emb = nn.Embedding(2, hidden_size)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 512, hidden_size))\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        # 嵌入融合\n",
    "        token_emb = self.embeddings(input_ids)\n",
    "        seg_emb = self.segment_emb(segment_ids)\n",
    "        pos_emb = self.pos_emb[:, :input_ids.size(1), :]\n",
    "        x = token_emb + seg_emb + pos_emb\n",
    "        # 编码器堆叠\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLM（Masked Language Model）：随机遮蔽15%的token（80%替换为[MASK]，10%随机替换，10%保留原词）\n",
    "class MaskedLM(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, hidden_states, masked_positions):\n",
    "        # 提取被遮蔽位置的隐藏状态\n",
    "        batch_size, seq_len, dim = hidden_states.shape\n",
    "        flat_positions = masked_positions.view(-1)\n",
    "        selected = hidden_states.view(-1, dim)[flat_positions]\n",
    "        # 解码预测\n",
    "        x = F.gelu(self.dense(selected))\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.decoder(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交叉熵损失  -P(x)logQ(x) P为真实分布，Q为预测分布\n",
    "import torch\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: 模型原始输出 (未归一化), shape: (N, C)\n",
    "    labels: 真实标签（类别索引）, shape: (N,)\n",
    "    \"\"\"\n",
    "    # 数值稳定性处理: 减去最大值防止指数爆炸\n",
    "    logits = logits - torch.max(logits, dim=1, keepdim=True)[0]\n",
    "    \n",
    "    # 计算LogSoftmax (等价于 log(softmax(logits)))\n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_probs = logits - torch.log(torch.sum(exp_logits, dim=1, keepdim=True))\n",
    "    \n",
    "    # 提取真实类别对应的对数概率\n",
    "    n_samples = logits.shape[0]\n",
    "    true_class_logprobs = log_probs[range(n_samples), labels]\n",
    "    \n",
    "    # 计算损失（平均损失）\n",
    "    loss = -torch.mean(true_class_logprobs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (984388660.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class Lora(nn.modules):\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Lora(nn.modules):\n",
    "    def __init__(self, in_dim, out_dim, rank=8, alpha = 16):\n",
    "        super().__init__\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "\n",
    "        self.scalling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scalling\n",
    "\n",
    "        return x @ (self.original_weight + delta_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.in_dim = original_layer.in_features\n",
    "        self.out_dim = original_layer.out_features\n",
    "\n",
    "        self.original_weight = original_layer.weight.clone().detach()  # 克隆原始权重并禁止梯度更新\n",
    "        self.original_weight.requires_grad_(False)\n",
    "        # 缩放因子：alpha / rank，用于平衡低秩更新的强度\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(self.in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, self.out_dim))\n",
    "\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算低秩更新 ΔW = A @ B * scaling\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scaling\n",
    "        # 将更新量叠加到原始权重（需从外部传入原始权重）\n",
    "        combined_weight = self.original_weight + delta_W\n",
    "        return F.linear(x, combined_weight, self.original_layer.bias)\n",
    "    \n",
    "original_linear = nn.Linear(768, 512)\n",
    "\n",
    "    # 创建LoRA适配层\n",
    "lora_layer = LoRALayer(original_linear, rank=8, alpha=16)\n",
    "\n",
    "    # 前向传播时直接替换使用\n",
    "x = torch.randn(32, 768)  # 输入维度\n",
    "output = lora_layer(x)    # 输出维度 (32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm代码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # 可学习参数γ\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1838695233.py, line 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 112\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"专家网络模块\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"MoE核心层\"\"\"\n",
    "    def __init__(self, d_model=768, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # 专家池初始化\n",
    "        self.experts = nn.ModuleList([Expert(d_model, d_model*4) for _ in range(num_experts)])\n",
    "        \n",
    "        # 门控网络\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "        \n",
    "        # 负载均衡辅助损失系数\n",
    "        self.aux_loss_coef = 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 门控网络计算\n",
    "        logits = self.gate(x)  # [B, S, E]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Top-K专家选择\n",
    "        topk_probs, topk_indices = probs.topk(self.top_k, dim=-1)  # [B, S, K]\n",
    "        topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 专家计算与组合\n",
    "        outputs = torch.zeros_like(x)   #创建形式相同的全0张量\n",
    "        for i in range(self.num_experts):\n",
    "            # 创建当前专家的mask\n",
    "            expert_mask = (topk_indices == i)\n",
    "            if expert_mask.any():\n",
    "                # 当前专家处理的token索引\n",
    "                batch_idx, seq_idx = torch.where(expert_mask)\n",
    "                \n",
    "                # 获取对应输入并计算专家输出\n",
    "                expert_input = x[batch_idx, seq_idx]\n",
    "                expert_output = self.experts[i](expert_input)\n",
    "                \n",
    "                # 加权累加结果\n",
    "                weights = topk_probs[batch_idx, seq_idx, expert_mask[expert_mask]]\n",
    "                outputs[batch_idx, seq_idx] += expert_output * weights.unsqueeze(-1)\n",
    "        \n",
    "        # 负载均衡损失计算\n",
    "        expert_mask = F.one_hot(topk_indices, self.num_experts).float()\n",
    "        expert_usage = expert_mask.mean(dim=0).mean(dim=0)\n",
    "        aux_loss = (expert_usage.std() + 1e-6) * self.aux_loss_coef\n",
    "        \n",
    "        return outputs, aux_loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
