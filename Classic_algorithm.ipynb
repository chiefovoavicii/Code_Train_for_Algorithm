{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerç¼–ç å™¨å±‚æ ¸å¿ƒä»£ç ï¼ˆPyTorchç¤ºä¾‹ï¼‰\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # è‡ªæ³¨æ„åŠ›è®¡ç®—\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
    "        src = src + self.norm1(src2)  # æ®‹å·®è¿æ¥+å½’ä¸€åŒ–\n",
    "        # å‰é¦ˆç½‘ç»œ\n",
    "        src2 = self.linear2(F.relu(self.linear1(src)))\n",
    "        src = src + self.norm2(src2)\n",
    "        return \n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = k.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / math.sqrt(dk)\n",
    "    if mask: scaled_attention_logits += mask * -1e9\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "    return torch.matmul(attention_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self_attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads=1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size #è¾“å…¥å‘é‡çš„ç»´åº¦\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3 * embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.shape\n",
    "        #ç”ŸæˆQ,K,V é€šè¿‡ä¸€æ¬¡çº¿æ€§å˜æ¢åˆ†å‰²æˆä¸‰éƒ¨åˆ†\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)  #åˆ†å‰²ä¸ºQ/K/V\n",
    "\n",
    "        #è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "        scale = (self.head_dim) ** -0.5  #ç¼©æ”¾å› å­æ ¹å·dï¼Œ é˜²æ­¢ç‚¹ç§¯è¿‡å¤§\n",
    "        attention_scores =  torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "        #åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return out, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2631102471.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    def forward(self, x):\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class multiattention(nn.Module):\n",
    "    def __init__(self, embed_size,head=8):\n",
    "        self.head = head\n",
    "        self.embed_size = embed_size\n",
    "        self.head_dim = embed_size // head\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3*embed_size)\n",
    "        self.to_out = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self,x):\n",
    "        batch_size, seq_len, embed_size = x.shape()\n",
    "        qkv = self.to_qkv(x).view(batch_size, seq_len, 3, self.head, self.head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4)\n",
    "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attention_scores = torch.matmul(q,k.transpose(-2,-1)) * self.head_dim ** -0.5\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention_weights, v)\n",
    "\n",
    "        out = out.permute(0,2,1,3).contiguous.view(batch_size, seq_len, embed_size)\n",
    "\n",
    "        out = self.to_out(out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadattention(nn.Module):\n",
    "    def __init__(self, head, embed_size):\n",
    "        super(multiattention(), self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.head = head\n",
    "        self.head_dim = embed_size // head\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size,3 * embed_size)\n",
    "        self.to_out = nn.Linear(embed_size,embed_size)\n",
    "    def forward(self,x):\n",
    "        batch_size, seq_len, embed_size = x.shape()\n",
    "        qkv = self.to_qkv(x).view(batch_size, seq_len, 3, self.head, self.head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4)\n",
    "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scale = self.head_dim ** -0.5\n",
    "        attention_score = torch.matmul(q, k.transpose(-2,-1)) * scale\n",
    "        attention_weight = F.softmax(attention_score, -1)\n",
    "\n",
    "        out = torch.matmul(attention_weight, v)\n",
    "        out = out.permute(0,2,1,3).contiguous().view(batch_size, seq_len, embed_size)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, head, d_model, group):\n",
    "        self.head = head\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // head\n",
    "        self.group = group\n",
    "\n",
    "        self.Wq = nn.linear(d_model, d_model)\n",
    "        self.Wk = nn.linear(d_model, d_model // group)\n",
    "        self.Wv = nn.linear(d_model, d_model)\n",
    "        self.out = nn.linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape()\n",
    "\n",
    "        q = self.Wq(x).view(batch_size, -1, self.head, self.head_dim)\n",
    "        k = self.Wq(x).view(batch_size, -1, self.head // self.group, self.head_dim)\n",
    "        v = self.Wq(x).view(batch_size, -1, self.head // self.group, self.head_dim)\n",
    "\n",
    "        scale = self.head_dim ** -0.5\n",
    "        attn_score = torch.matmul(q,k.transpose(-2,-1)) * scale\n",
    "        attn_weight = F.softmax(attn_score, -1)\n",
    "\n",
    "        out = torch.matmul(attn_weight, v)\n",
    "\n",
    "        out = out.permute(0,2,1,3).contiguous.view(batch_size, seq_len, d_model)\n",
    "\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module): # ç±»åæ”¹ä¸ºé©¼å³°å¼\n",
    "    def __init__(self, head, embed_size):\n",
    "        # 1. ä¿®æ­£super()çš„è°ƒç”¨\n",
    "        super().__init__() # æˆ–è€… super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.head = head\n",
    "        self.head_dim = embed_size // head\n",
    "        assert self.head_dim * head == embed_size, \"embed_size å¿…é¡»èƒ½è¢« head æ•´é™¤\"\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3 * embed_size)\n",
    "        self.to_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # è¾“å…¥ x çš„ shape: [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        # 2. ä¿®æ­£ .shape() ä¸º .shape\n",
    "        batch_size, seq_len, embed_size = x.shape\n",
    "\n",
    "        # çº¿æ€§å˜æ¢å¹¶é‡å¡‘\n",
    "        # self.to_qkv(x) -> shape: [batch_size, seq_len, 3 * embed_size]\n",
    "        qkv = self.to_qkv(x).view(batch_size, seq_len, 3, self.head, self.head_dim)\n",
    "        # qkv shape: [batch_size, seq_len, 3, head, head_dim]\n",
    "\n",
    "        # ç»´åº¦é‡æ’ï¼Œå°† q, k, v åˆ†ç¦»\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # qkv shape: [3, batch_size, head, seq_len, head_dim]\n",
    "\n",
    "        # åˆ†ç¦» q, k, v\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        # q, k, v çš„ shape å‡ä¸º: [batch_size, head, seq_len, head_dim]\n",
    "\n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        scale = self.head_dim ** -0.5\n",
    "        attention_score = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        # k.transpose(-2, -1) -> shape: [batch_size, head, head_dim, seq_len]\n",
    "        # attention_score shape: [batch_size, head, seq_len, seq_len]\n",
    "\n",
    "        attention_weight = F.softmax(attention_score, dim=-1)\n",
    "        # attention_weight shape: [batch_size, head, seq_len, seq_len]\n",
    "\n",
    "        # æ³¨æ„åŠ›æƒé‡ä¸ v ç›¸ä¹˜\n",
    "        out = torch.matmul(attention_weight, v)\n",
    "        # out shape: [batch_size, head, seq_len, head_dim]\n",
    "\n",
    "        # åˆå¹¶å¤šå¤´\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, embed_size)\n",
    "        # out.permute(...) -> shape: [batch_size, seq_len, head, head_dim]\n",
    "        # out.view(...) -> shape: [batch_size, seq_len, embed_size]\n",
    "\n",
    "        out = self.to_out(out)\n",
    "        # out shape: [batch_size, seq_len, embed_size]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(self, head, d_model, group):\n",
    "        super().__init__() # ä¿®æ­£super()\n",
    "        self.head = head\n",
    "        self.d_model = d_model\n",
    "        self.group = group\n",
    "        self.kv_heads = head // group # Kå’ŒVçš„å¤´æ•°\n",
    "        self.head_dim = d_model // head\n",
    "        \n",
    "        assert head % group == 0, \"head å¿…é¡»èƒ½è¢« group æ•´é™¤\"\n",
    "\n",
    "        # 1. ä¿®æ­£ nn.linear\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        # Kå’ŒVçš„æŠ•å½±ç»´åº¦æ˜¯ kv_heads * head_dim\n",
    "        self.Wk = nn.Linear(d_model, self.kv_heads * self.head_dim)\n",
    "        self.Wv = nn.Linear(d_model, self.kv_heads * self.head_dim)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # è¾“å…¥ x çš„ shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 2. ä¿®æ­£ .shape()\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # æŠ•å½±å¹¶é‡å¡‘ Q, K, V\n",
    "        q = self.Wq(x).view(batch_size, seq_len, self.head, self.head_dim)\n",
    "        # q shape: [batch_size, seq_len, head, head_dim]\n",
    "        \n",
    "        # 3. ä¿®æ­£ K, V çš„æŠ•å½±\n",
    "        k = self.Wk(x).view(batch_size, seq_len, self.kv_heads, self.head_dim)\n",
    "        # k shape: [batch_size, seq_len, kv_heads, head_dim]\n",
    "        \n",
    "        v = self.Wv(x).view(batch_size, seq_len, self.kv_heads, self.head_dim)\n",
    "        # v shape: [batch_size, seq_len, kv_heads, head_dim]\n",
    "\n",
    "        # ç»´åº¦é‡æ’ä»¥è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—\n",
    "        q = q.transpose(1, 2) # -> [batch_size, head, seq_len, head_dim]\n",
    "        k = k.transpose(1, 2) # -> [batch_size, kv_heads, seq_len, head_dim]\n",
    "        v = v.transpose(1, 2) # -> [batch_size, kv_heads, seq_len, head_dim]\n",
    "\n",
    "        # 5. GQAæ ¸å¿ƒï¼šé‡å¤Kå’ŒVçš„å¤´æ¥åŒ¹é…Qçš„å¤´\n",
    "        # [B, kv, S, D] -> [B, kv, 1, S, D] -> [B, kv, group, S, D] -> [B, head, S, D]\n",
    "        k = k.unsqueeze(2).repeat(1, 1, self.group, 1, 1).view(batch_size, self.head, seq_len, self.head_dim)\n",
    "        v = v.unsqueeze(2).repeat(1, 1, self.group, 1, 1).view(batch_size, self.head, seq_len, self.head_dim)\n",
    "        # ç°åœ¨ q, k, v çš„ shape å‡ä¸º: [batch_size, head, seq_len, head_dim]\n",
    "\n",
    "        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŒMHAï¼‰\n",
    "        scale = self.head_dim ** -0.5\n",
    "        attn_score = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        # attn_score shape: [batch_size, head, seq_len, seq_len]\n",
    "        \n",
    "        attn_weight = F.softmax(attn_score, dim=-1)\n",
    "        # attn_weight shape: [batch_size, head, seq_len, seq_len]\n",
    "\n",
    "        out = torch.matmul(attn_weight, v)\n",
    "        # out shape: [batch_size, head, seq_len, head_dim]\n",
    "\n",
    "        # åˆå¹¶å¤šå¤´\n",
    "        # 6. ä¿®æ­£ contiguous\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        # out.transpose(...) -> shape: [batch_size, seq_len, head, head_dim]\n",
    "        # out.view(...) -> shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        out = self.out(out)\n",
    "        # out shape: [batch_size, seq_len, d_model]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) å±‚ã€‚\n",
    "    \n",
    "    è¯¥å±‚é€šè¿‡åœ¨åŸå§‹æƒé‡çŸ©é˜µä¸Šå åŠ ä¸€ä¸ªä½ç§©æ›´æ–°çŸ©é˜µ (Î”W = A * B) æ¥å®ç°å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒã€‚\n",
    "    åœ¨è®­ç»ƒæœŸé—´ï¼Œåªæœ‰ä½ç§©çŸ©é˜µ A å’Œ B æ˜¯å¯è®­ç»ƒçš„ï¼ŒåŸå§‹æƒé‡ä¿æŒå†»ç»“ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, original_layer: nn.Linear, rank: int = 8, alpha: int = 16):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– LoRA å±‚ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            original_layer (nn.Linear): è¦è¢«é€‚é…çš„åŸå§‹çº¿æ€§å±‚ã€‚\n",
    "            rank (int): ä½ç§©çŸ©é˜µçš„ç§©ï¼Œå³çŸ©é˜µ A å’Œ B çš„ä¸­é—´ç»´åº¦ã€‚\n",
    "            alpha (int): ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´ä½ç§©æ›´æ–°çš„å¼ºåº¦ã€‚\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # ä»åŸå§‹å±‚è·å–è¾“å…¥å’Œè¾“å‡ºç»´åº¦\n",
    "        self.in_dim = original_layer.in_features\n",
    "        self.out_dim = original_layer.out_features\n",
    "\n",
    "        # å¤åˆ¶åŸå§‹æƒé‡ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºä¸å¯è®­ç»ƒï¼ˆå†»ç»“ï¼‰\n",
    "        self.original_weight = original_layer.weight.clone().detach()\n",
    "        self.original_weight.requires_grad_(False)\n",
    "        \n",
    "        # å¤åˆ¶åŸå§‹åç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¹¶å†»ç»“\n",
    "        self.original_bias = None\n",
    "        if original_layer.bias is not None:\n",
    "            self.original_bias = original_layer.bias.clone().detach()\n",
    "            self.original_bias.requires_grad_(False)\n",
    "\n",
    "        # åˆå§‹åŒ–ä½ç§©çŸ©é˜µ A å’Œ B\n",
    "        # A ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼ŒB åˆå§‹åŒ–ä¸ºé›¶\n",
    "        self.A = nn.Parameter(torch.randn(self.in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, self.out_dim))\n",
    "\n",
    "        # è®¡ç®—ç¼©æ”¾å› å­\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            x (torch.Tensor): è¾“å…¥å¼ é‡ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "            torch.Tensor: è¾“å‡ºå¼ é‡ã€‚\n",
    "        \"\"\"\n",
    "        # è®¡ç®—ä½ç§©æ›´æ–°çŸ©é˜µ Î”W = A @ B * scaling\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scaling\n",
    "        \n",
    "        # å°†ä½ç§©æ›´æ–°å åŠ åˆ°åŸå§‹æƒé‡ä¸Š\n",
    "        combined_weight = self.original_weight + delta_W\n",
    "        \n",
    "        # ä½¿ç”¨ç»„åˆåçš„æƒé‡æ‰§è¡Œçº¿æ€§å˜æ¢\n",
    "        return F.linear(x, combined_weight, self.original_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lora(nn.Module):\n",
    "    def __init__(self, original_layer, rank,alpha):\n",
    "        super().__init__()\n",
    "        self.in_dim = original_layer.in_features\n",
    "        self.out_dim = original_layer.out_features\n",
    "\n",
    "        self.original_weights = original_layer.weights.clone().detach()\n",
    "        self.original_weights.requires_grad_(False)\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(self.in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, self.out_dim))\n",
    "\n",
    "        self.scale = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scale\n",
    "\n",
    "        combined_weights = self.original_weights + delta_W\n",
    "\n",
    "        return F.linear(x, combined_weights, self.original_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QLoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    QLoRA (Quantized Low-Rank Adaptation) å±‚ã€‚\n",
    "\n",
    "    è¯¥å±‚åœ¨ LoRA çš„åŸºç¡€ä¸Šå¢åŠ äº†é‡åŒ–æ­¥éª¤ã€‚å®ƒé¦–å…ˆè®¡ç®—ä½ç§©æ›´æ–°çŸ©é˜µ Î”Wï¼Œ\n",
    "    ç„¶åå¯¹ Î”W è¿›è¡Œé‡åŒ–ï¼Œæœ€åå°†é‡åŒ–åçš„æ›´æ–°å åŠ åˆ°åŸå§‹æƒé‡ä¸Šã€‚\n",
    "    è¿™è¿›ä¸€æ­¥å‡å°‘äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, original_layer: nn.Linear, rank: int = 8, alpha: int = 16, quantization_bits: int = 8):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– QLoRA å±‚ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            original_layer (nn.Linear): è¦è¢«é€‚é…çš„åŸå§‹çº¿æ€§å±‚ã€‚\n",
    "            rank (int): ä½ç§©çŸ©é˜µçš„ç§©ã€‚\n",
    "            alpha (int): ç¼©æ”¾å› å­ã€‚\n",
    "            quantization_bits (int): ç”¨äºé‡åŒ–ä½ç§©æ›´æ–°çš„ä½æ•°ã€‚\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.quantization_bits = quantization_bits\n",
    "        \n",
    "        # ä»åŸå§‹å±‚è·å–è¾“å…¥å’Œè¾“å‡ºç»´åº¦\n",
    "        self.in_dim = original_layer.in_features\n",
    "        self.out_dim = original_layer.out_features\n",
    "\n",
    "        # å¤åˆ¶åŸå§‹æƒé‡å’Œåç½®ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºä¸å¯è®­ç»ƒï¼ˆå†»ç»“ï¼‰\n",
    "        self.original_weight = original_layer.weight.clone().detach()\n",
    "        self.original_weight.requires_grad_(False)\n",
    "        \n",
    "        self.original_bias = None\n",
    "        if original_layer.bias is not None:\n",
    "            self.original_bias = original_layer.bias.clone().detach()\n",
    "            self.original_bias.requires_grad_(False)\n",
    "\n",
    "        # åˆå§‹åŒ–ä½ç§©çŸ©é˜µ A å’Œ B\n",
    "        self.A = nn.Parameter(torch.randn(self.in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, self.out_dim))\n",
    "\n",
    "        # è®¡ç®—ç¼©æ”¾å› å­\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def quantize(self, weight: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å¯¹æƒé‡è¿›è¡Œä¸€ä¸ªç®€å•çš„å¯¹ç§°é‡åŒ–ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            weight (torch.Tensor): è¦é‡åŒ–çš„æƒé‡å¼ é‡ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "            torch.Tensor: é‡åŒ–åçš„æƒé‡å¼ é‡ã€‚\n",
    "        \"\"\"\n",
    "        # è®¡ç®—é‡åŒ–èŒƒå›´\n",
    "        q_range = 2 ** (self.quantization_bits - 1)\n",
    "        \n",
    "        # æ‰¾åˆ°æƒé‡çš„æœ€å¤§ç»å¯¹å€¼ä½œä¸ºç¼©æ”¾åŸºå‡†\n",
    "        scale = q_range / torch.max(torch.abs(weight))\n",
    "        \n",
    "        # é‡åŒ–ï¼šç¼©æ”¾ã€å››èˆäº”å…¥ã€è£å‰ªèŒƒå›´\n",
    "        quantized_weight = (weight * scale).round().clamp(-q_range, q_range - 1)\n",
    "        \n",
    "        # åé‡åŒ–ï¼šå°†é‡åŒ–åçš„æ•´æ•°å€¼è½¬æ¢å›æµ®ç‚¹æ•°\n",
    "        dequantized_weight = quantized_weight / scale\n",
    "        return dequantized_weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            x (torch.Tensor): è¾“å…¥å¼ é‡ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "            torch.Tensor: è¾“å‡ºå¼ é‡ã€‚\n",
    "        \"\"\"\n",
    "        # è®¡ç®—ä½ç§©æ›´æ–°çŸ©é˜µ Î”W\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scaling\n",
    "        \n",
    "        # å¯¹ä½ç§©æ›´æ–°è¿›è¡Œé‡åŒ–å’Œåé‡åŒ–\n",
    "        quantized_delta_W = self.quantize(delta_W)\n",
    "        \n",
    "        # å°†é‡åŒ–åçš„æ›´æ–°å åŠ åˆ°åŸå§‹æƒé‡ä¸Š\n",
    "        combined_weight = self.original_weight + quantized_delta_W\n",
    "        \n",
    "        # ä½¿ç”¨ç»„åˆåçš„æƒé‡æ‰§è¡Œçº¿æ€§å˜æ¢\n",
    "        return F.linear(x, combined_weight, self.original_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3180739669.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    assert embed_size % heads == 0, #embed_size å¿…é¡»èƒ½è¢« heads æ•´é™¤\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#MultiAttention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        assert embed_size % heads == 0, #embed_size å¿…é¡»èƒ½è¢« heads æ•´é™¤\n",
    "        self.heads_dim = embed_size // heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(embed_size, 3 * embed_size)\n",
    "        self.to_out = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.shape\n",
    "\n",
    "        qkv = self.to_qkv(x).view(batch_size, seq_len, 3, self.heads, self.heads_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # è°ƒæ•´ç»´åº¦é¡ºåºä¸º (3, batch_size, heads, seq_len, heads_dim)  #æ–¹ä¾¿ä¹‹åKçŸ©é˜µçš„è½¬ç½®\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # åˆ†ç¦»ä¸ºæŸ¥è¯¢ qã€é”® kã€å€¼ vï¼Œæ¯ä¸ªå½¢çŠ¶ä¸º (batch_size, heads, seq_len, heads_dim)\n",
    "        #qï¼šbatch_size, heads, seq_len, heads_dim\n",
    "        scale = self.heads_dim ** -0.5  # è®¡ç®—ç¼©æ”¾å› å­ï¼Œé˜²æ­¢ç‚¹ç§¯è¿‡å¤§ï¼Œç­‰äº 1/sqrt(heads_dim)\n",
    "        attention_scores = torch.matmul(q,k.transpose(-2, -1)) * scale\n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šq å’Œ k çš„è½¬ç½®ç‚¹ç§¯åä¹˜ä»¥ç¼©æ”¾å› å­\n",
    "        # k.transpose(-2, -1) å°† k çš„æœ€åä¸¤ä¸ªç»´åº¦ (seq_len, heads_dim) è½¬ç½®ä¸º (heads_dim, seq_len)\n",
    "        # ç»“æœå½¢çŠ¶ä¸º (batch_size, heads, seq_len, seq_len)\n",
    "\n",
    "        attention_wights = F.softmax(attention_scores, dim=-1)  # å¯¹æ³¨æ„åŠ›åˆ†æ•°åº”ç”¨ softmaxï¼Œå¾—åˆ°æƒé‡\n",
    "        # dim=-1 è¡¨ç¤ºæ²¿æœ€åä¸€ä¸ªç»´åº¦ï¼ˆåºåˆ—é•¿åº¦ï¼‰å½’ä¸€åŒ–ï¼Œå½¢çŠ¶ä¿æŒ (batch_size, heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attention_wights, v)\n",
    "        # attention_weights å’Œ v çš„çŸ©é˜µä¹˜æ³•ï¼Œç»“æœå½¢çŠ¶ä¸º (batch_size, heads, seq_len, heads_dim)\n",
    "        print(out)\n",
    "        #ç”¨äºåˆå¹¶æœ€åä¸¤ä¸ªç»´åº¦heads*heads_dim\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, embed_size)\n",
    "        #permuteåï¼Œbatch_size, seq_len, heads, heads_dim\n",
    "        # contiguous() ç¡®ä¿å†…å­˜è¿ç»­ï¼Œview é‡å¡‘ä¸º (batch_size, seq_len, embed_size)\n",
    "        print(out)\n",
    "        #batch_size, seq_len, embed_size\n",
    "        out = self.to_out(out)\n",
    "        print(out)\n",
    "        return out   \n",
    "    \n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, num_head, groups, k_v_heads, embed_size):\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_size // num_head\n",
    "        self.num_head = num_head\n",
    "        self.groups = groups\n",
    "        self.k_v_heads = k_v_heads\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
    "        self.k_proj = nn.Linear(embed_size, k_v_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_size, k_v_heads * self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, head_dim = x.shape\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.groups, self.num_head, head_dim)\n",
    "        q = q.permute(0,2,3,1,4)\n",
    "        k = k.view(batch_size, seq_len, self.num_head, head_dim)\n",
    "        k = k.permute(0,2,3,1)\n",
    "        v = v.view(batch_size, seq_len, self.num_head, head_dim)\n",
    "        v = v.permute(0,2,1,3)\n",
    "\n",
    "        scale = self.head_dim ** -0.5\n",
    "        score = torch.matmul(q,k) * scale   # q @ k\n",
    "\n",
    "        weight = F.softmax(score, dim=-1) # (batch, groups, num_head, seq_len, seq_len)\n",
    "\n",
    "        out = torch.matmul(weight, v)\n",
    "\n",
    "        out = out.permute(0, 3, 1, 2, 4)\n",
    "        \n",
    "        out = out.contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, num_head, group, kv_heads, embed_size):\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_size // num_head\n",
    "        self.group = group\n",
    "        self.num_head = num_head\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
    "        self.k_proj = nn.Linear(embed_size, kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_size, kv_heads * self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.shape()\n",
    "        \n",
    "        q = self.q_proj(x)\n",
    "        q = q.view(batch_size, seq_len, self.group, self.num_head, self.head_dim)\n",
    "        k = self.k_proj(x)\n",
    "        k = k.view(batch_size, seq_len, self.num_head, self.head_dim)\n",
    "        v = self.v_proj(x)\n",
    "        v = v.view(batch_size, seq_len, self.num_head, self.head_dim)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "# è¾“å…¥å½¢çŠ¶: (batch_size, seq_len, embed_size)\n",
    "# BNç»Ÿè®¡é‡åœ¨ (batch_size, seq_len) ä¸Šè®¡ç®—ï¼Œå¯¹æ¯ä¸ª embed_size é€šé“ç‹¬ç«‹å½’ä¸€åŒ–\n",
    "bn = nn.BatchNorm1d(embed_size)\n",
    "x_bn = bn(x)  # è¾“å‡ºå½¢çŠ¶ä»ä¸º (batch_size, seq_len, embed_size)\n",
    "\n",
    "# è¾“å…¥å½¢çŠ¶: (batch_size, seq_len, embed_size)\n",
    "# LNç»Ÿè®¡é‡åœ¨ embed_size ä¸Šè®¡ç®—ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªä½ç½®ç‹¬ç«‹å½’ä¸€åŒ–\n",
    "ln = nn.LayerNorm(embed_size)\n",
    "x_ln = ln(x)  # è¾“å‡ºå½¢çŠ¶ä»ä¸º (batch_size, seq_len, embed_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GRPO æœ€å°å®ç° - ä½“ç°æ ¸å¿ƒæ€æƒ³\n",
    "åªä¿ç•™ï¼šç»„å†…ä¼˜åŠ¿è®¡ç®— + PPOè£å‰ª + é‡è¦æ€§é‡‡æ ·\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict\n",
    "\n",
    "class GRPO:\n",
    "    \"\"\"\n",
    "    GRPOæ ¸å¿ƒå®ç°\n",
    "    \n",
    "    æ ¸å¿ƒå…¬å¼ï¼š\n",
    "    1. ç»„å†…ä¼˜åŠ¿ï¼šadvantage = (reward - group_mean) / group_std\n",
    "    2. PPOæŸå¤±ï¼šL = -min(r*A, clip(r, 1-Îµ, 1+Îµ)*A)\n",
    "       å…¶ä¸­ r = exp(log_Ï€_new - log_Ï€_old) (é‡è¦æ€§é‡‡æ ·æ¯”ç‡)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_net: nn.Module,\n",
    "        lr: float = 1e-4,\n",
    "        epsilon: float = 0.2,    # PPOè£å‰ªèŒƒå›´\n",
    "        group_size: int = 4,     # ç»„å¤§å°\n",
    "        num_epochs: int = 4      # æ›´æ–°è½®æ•°\n",
    "    ):\n",
    "        self.policy = policy_net\n",
    "        self.optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "        self.epsilon = epsilon\n",
    "        self.group_size = group_size\n",
    "        self.num_epochs = num_epochs\n",
    "    \n",
    "    def compute_group_advantages(self, rewards: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ç»„å†…ä¼˜åŠ¿è®¡ç®— (GRPOæ ¸å¿ƒ)\n",
    "        \n",
    "        è¾“å…¥: rewards [batch_size] - æ¯ä¸ªæ ·æœ¬çš„å¥–åŠ±\n",
    "        è¾“å‡º: advantages [batch_size] - ç»„å†…å½’ä¸€åŒ–çš„ä¼˜åŠ¿\n",
    "        \n",
    "        ç¤ºä¾‹:\n",
    "        rewards = [1.0, 2.0, 3.0, 4.0,  10.0, 11.0, 12.0, 13.0]\n",
    "                   â””â”€â”€â”€â”€ç»„1â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€ç»„2â”€â”€â”€â”€â”€â”€â”˜\n",
    "        \n",
    "        ç»„1ä¼˜åŠ¿ â‰ˆ [-1.34, -0.45, 0.45, 1.34]  # ç»„å†…æ ‡å‡†åŒ–\n",
    "        ç»„2ä¼˜åŠ¿ â‰ˆ [-1.34, -0.45, 0.45, 1.34]  # ç»„å†…æ ‡å‡†åŒ–\n",
    "        \"\"\"\n",
    "        batch_size = len(rewards)\n",
    "        num_groups = batch_size // self.group_size\n",
    "        \n",
    "        # é‡å¡‘ä¸º [num_groups, group_size]\n",
    "        rewards_grouped = rewards[:num_groups * self.group_size].view(\n",
    "            num_groups, self.group_size\n",
    "        )\n",
    "        \n",
    "        # ç»„å†…æ ‡å‡†åŒ–\n",
    "        mean = rewards_grouped.mean(dim=1, keepdim=True)\n",
    "        std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8\n",
    "        advantages = (rewards_grouped - mean) / std\n",
    "        \n",
    "        return advantages.view(-1)  # å±•å¹³ä¸º [batch_size]\n",
    "    \n",
    "    def ppo_loss(\n",
    "        self, \n",
    "        old_log_probs: torch.Tensor,  # æ—§ç­–ç•¥å¯¹æ•°æ¦‚ç‡\n",
    "        new_log_probs: torch.Tensor,  # æ–°ç­–ç•¥å¯¹æ•°æ¦‚ç‡\n",
    "        advantages: torch.Tensor       # ä¼˜åŠ¿å‡½æ•°\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        PPOè£å‰ªæŸå¤±\n",
    "        \n",
    "        ratio = Ï€_new / Ï€_old = exp(log Ï€_new - log Ï€_old)\n",
    "        L = -E[min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A)]\n",
    "        \"\"\"\n",
    "        # é‡è¦æ€§é‡‡æ ·æ¯”ç‡\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        \n",
    "        # ä¸¤ä¸ªç›®æ ‡å‡½æ•°\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "        \n",
    "        # å–æ‚²è§‚ä¼°è®¡ï¼ˆæœ€å°å€¼ï¼‰\n",
    "        return -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        sequences: torch.Tensor,      # [batch_size, seq_len]\n",
    "        old_log_probs: torch.Tensor,  # [batch_size]\n",
    "        rewards: torch.Tensor         # [batch_size]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        ä¸€æ­¥è®­ç»ƒ\n",
    "        \n",
    "        æµç¨‹:\n",
    "        1. è®¡ç®—ç»„å†…ä¼˜åŠ¿ï¼ˆä¸éœ€è¦ä»·å€¼ç½‘ç»œï¼‰\n",
    "        2. å¤šè½®PPOæ›´æ–°\n",
    "        \"\"\"\n",
    "        # æ­¥éª¤1: ç»„å†…ä¼˜åŠ¿\n",
    "        advantages = self.compute_group_advantages(rewards)\n",
    "        \n",
    "        # æ­¥éª¤2: å¤šè½®æ›´æ–°\n",
    "        total_loss = 0.0\n",
    "        for _ in range(self.num_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # è·å–æ–°ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡\n",
    "            new_log_probs = self.policy.get_log_probs(sequences)\n",
    "            \n",
    "            # PPOæŸå¤±\n",
    "            loss = self.ppo_loss(old_log_probs, new_log_probs, advantages)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / self.num_epochs,\n",
    "            'reward_mean': rewards.mean().item()\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "# ============================================================================\n",
    "\n",
    "class SimplePolicy(nn.Module):\n",
    "    \"\"\"ç®€å•ç­–ç•¥ç½‘ç»œï¼ˆç¤ºä¾‹ï¼‰\"\"\"\n",
    "    def __init__(self, vocab_size=100, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, hidden),\n",
    "            nn.LSTM(hidden, hidden, batch_first=True),\n",
    "            nn.Linear(hidden, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def get_log_probs(self, sequences):\n",
    "        \"\"\"è®¡ç®—åºåˆ—å¯¹æ•°æ¦‚ç‡\"\"\"\n",
    "        # ç®€åŒ–å®ç°ï¼šè¿”å›éšæœºå€¼ï¼ˆå®é™…åº”è®¡ç®—çœŸå®æ¦‚ç‡ï¼‰\n",
    "        return torch.randn(sequences.size(0))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"GRPO æœ€å°å®ç°æ¼”ç¤º\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # åˆå§‹åŒ–\n",
    "    policy = SimplePolicy()\n",
    "    grpo = GRPO(policy, lr=1e-4, epsilon=0.2, group_size=4)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿæ•°æ®\n",
    "    batch_size = 8\n",
    "    seq_len = 10\n",
    "    \n",
    "    sequences = torch.randint(0, 100, (batch_size, seq_len))\n",
    "    old_log_probs = torch.randn(batch_size)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¥–åŠ±ï¼šç»„1ä½åˆ†ï¼Œç»„2é«˜åˆ†\n",
    "    rewards = torch.tensor([\n",
    "        2.0, 2.1, 2.2, 2.3,  # ç»„1ï¼ˆä½è´¨é‡ï¼‰\n",
    "        8.0, 8.1, 8.2, 8.3   # ç»„2ï¼ˆé«˜è´¨é‡ï¼‰\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n1. åŸå§‹å¥–åŠ±\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"ç»„1: {rewards[:4].tolist()}\")\n",
    "    print(f\"ç»„2: {rewards[4:].tolist()}\")\n",
    "    \n",
    "    print(\"\\n2. ç»„å†…ä¼˜åŠ¿ï¼ˆGRPOæ ¸å¿ƒï¼‰\")\n",
    "    print(\"-\" * 70)\n",
    "    advantages = grpo.compute_group_advantages(rewards)\n",
    "    print(f\"ç»„1ä¼˜åŠ¿: {[f'{x:.2f}' for x in advantages[:4].tolist()]}\")\n",
    "    print(f\"ç»„2ä¼˜åŠ¿: {[f'{x:.2f}' for x in advantages[4:].tolist()]}\")\n",
    "    print(\"\\nâœ… å…³é”®è§‚å¯Ÿï¼š\")\n",
    "    print(\"   - ä¸¤ç»„ä¼˜åŠ¿åˆ†å¸ƒç›¸åŒï¼ˆéƒ½æ˜¯ä»è´Ÿåˆ°æ­£ï¼‰\")\n",
    "    print(\"   - ç»„å†…ç›¸å¯¹æ’åºè¢«ä¿ç•™ï¼Œç»å¯¹å€¼è¢«æ¶ˆé™¤\")\n",
    "    \n",
    "    print(\"\\n3. æ‰§è¡Œè®­ç»ƒ\")\n",
    "    print(\"-\" * 70)\n",
    "    metrics = grpo.train_step(sequences, old_log_probs, rewards)\n",
    "    print(f\"æŸå¤±: {metrics['loss']:.4f}\")\n",
    "    print(f\"å¹³å‡å¥–åŠ±: {metrics['reward_mean']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ’¡ GRPO vs PPO å¯¹æ¯”\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "    PPOä¼˜åŠ¿è®¡ç®—:\n",
    "      advantage = R + Î³V(s') - V(s)  # éœ€è¦ä»·å€¼ç½‘ç»œV\n",
    "      å…¨å±€æ ‡å‡†åŒ–: (A - mean_all) / std_all\n",
    "    \n",
    "    GRPOä¼˜åŠ¿è®¡ç®—:\n",
    "      advantage = (R - mean_group) / std_group  # åªéœ€å¥–åŠ±R\n",
    "      ç»„å†…æ ‡å‡†åŒ–: æ¯ç»„ç‹¬ç«‹å½’ä¸€åŒ–\n",
    "    \n",
    "    GRPOä¼˜åŠ¿:\n",
    "      âœ… ä¸éœ€è¦ä»·å€¼ç½‘ç»œï¼ˆçœå†…å­˜ï¼‰\n",
    "      âœ… é¿å…ä»·å€¼ä¼°è®¡è¯¯å·®\n",
    "      âœ… æ›´ç¨³å®šï¼ˆç»„é—´å·®å¼‚è¢«æ¶ˆé™¤ï¼‰\n",
    "      âœ… é€‚åˆç¨€ç–å¥–åŠ±ä»»åŠ¡\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim = 1):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "@register_policy_loss(\"gspo\")\n",
    "def compute_policy_loss_gspo(\n",
    "    old_log_prob: torch.Tensor,      # æ—§ç­–ç•¥ä¸‹ï¼Œæ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡ï¼Œå½¢çŠ¶: [batch_size, response_length]\n",
    "    log_prob: torch.Tensor,          # æ–°ç­–ç•¥ä¸‹ï¼Œæ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡ï¼Œå½¢çŠ¶: [batch_size, response_length]\n",
    "    advantages: torch.Tensor,        # æ¯ä¸ª token çš„ä¼˜åŠ¿å€¼ A_i,tï¼Œå½¢çŠ¶: [batch_size, response_length]\n",
    "    response_mask: torch.Tensor,     # å“åº” token çš„ maskï¼ˆ1 è¡¨ç¤ºå‚ä¸æŸå¤±ï¼Œ0 è¡¨ç¤ºå¿½ç•¥ï¼‰ï¼Œå½¢çŠ¶: [batch_size, response_length]\n",
    "    loss_agg_mode: str = \"seq-mean-token-mean\",  # æŸå¤±èšåˆæ–¹å¼ï¼Œæ¨èä½¿ç”¨ seq-mean-token-mean\n",
    "    config: Optional[DictConfig | ActorConfig] = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    è®¡ç®— GSPO çš„æˆªæ–­ç­–ç•¥æ¢¯åº¦ç›®æ ‡ä»¥åŠç›¸å…³æŒ‡æ ‡ã€‚\n",
    "\n",
    "    GSPO æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "      * å…ˆåœ¨â€œåºåˆ—çº§åˆ«â€ä¸Šå®šä¹‰ä¸€ä¸ªé‡è¦æ€§é‡‡æ ·æ¯”ç‡ s_i(Î¸)ï¼Œå†æŠŠå®ƒåˆ†é…åˆ°æ¯ä¸ª tokenã€‚\n",
    "      * æœ€ç»ˆåœ¨ token çº§åˆ«åšç±»ä¼¼ PPO çš„è£å‰ªï¼Œä»è€Œç¨³å®šè®­ç»ƒã€‚\n",
    "\n",
    "    å‚æ•°è¯´æ˜:\n",
    "        old_log_prob (torch.Tensor):\n",
    "            æ—§ç­–ç•¥ Ï€_old å¯¹æ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡ï¼Œå½¢çŠ¶ (batch_size, response_length)ã€‚\n",
    "        log_prob (torch.Tensor):\n",
    "            å½“å‰ç­–ç•¥ Ï€_Î¸ å¯¹æ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡ï¼Œå½¢çŠ¶ (batch_size, response_length)ã€‚\n",
    "        advantages (torch.Tensor):\n",
    "            æ¯ä¸ª token çš„ä¼˜åŠ¿ A_i,tï¼Œå½¢çŠ¶ (batch_size, response_length)ã€‚\n",
    "        response_mask (torch.Tensor):\n",
    "            å“åº” token æ©ç ï¼Œ1 è¡¨ç¤ºè¯¥ä½ç½®å‚ä¸æŸå¤±ï¼Œ0 è¡¨ç¤ºä¸å‚ä¸ã€‚\n",
    "        loss_agg_mode (str, optional):\n",
    "            ç”¨äº agg_loss çš„èšåˆæ¨¡å¼ã€‚GSPO æ¨è \"seq-mean-token-mean\"ï¼š\n",
    "            å…ˆå¯¹ä¸€ä¸ªåºåˆ—çš„ token åšå¹³å‡ï¼Œå†å¯¹ batch ä¸­çš„æ‰€æœ‰åºåˆ—åšå¹³å‡ã€‚\n",
    "        config:\n",
    "            åŒ…å«è£å‰ªç³»æ•°ç­‰è¶…å‚æ•°çš„é…ç½®å¯¹è±¡ï¼Œéœ€æä¾› clip_ratio æˆ– clip_ratio_{low,high}ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "        pg_loss:        ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆæ ‡é‡ï¼‰ã€‚\n",
    "        pg_clipfrac:    è¢«è£å‰ªçš„æ¯”ä¾‹ï¼ˆä¸Š/ä¸‹ç•Œåˆåœ¨ä¸€èµ·ç»Ÿè®¡ï¼‰ã€‚\n",
    "        ppo_kl:         è¿‘ä¼¼ KL (å¯¹æ•°æ¯”ç‡çš„è´Ÿå·åŠ æƒå¹³å‡)ï¼Œç”¨äºç›‘æ§ç­–ç•¥æ›´æ–°æ­¥é•¿ã€‚\n",
    "        pg_clipfrac_lower: ä¸ºå…¼å®¹æ¥å£é¢„ç•™çš„ä¸‹ç•Œè£å‰ªæ¯”ä¾‹ï¼Œè¿™é‡Œå›ºå®šä¸º 0ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # å¿…é¡»æä¾› configï¼ˆé‡Œé¢åŒ…å«è£å‰ªèŒƒå›´ç­‰è¶…å‚æ•°ï¼‰\n",
    "    assert config is not None\n",
    "    # å¦‚æœé…ç½®ä¸­æä¾›äº† clip_ratio_low / clip_ratio_highï¼Œåˆ™ä½¿ç”¨å®ƒä»¬ï¼›\n",
    "    # å¦åˆ™é€€åŒ–ä¸ºå¯¹ç§°è£å‰ª [-clip_ratio, +clip_ratio]ã€‚\n",
    "    clip_ratio_low = config.clip_ratio_low if config.clip_ratio_low is not None else config.clip_ratio\n",
    "    clip_ratio_high = config.clip_ratio_high if config.clip_ratio_high is not None else config.clip_ratio\n",
    "\n",
    "    # negative_approx_kl = log Ï€_new - log Ï€_old\n",
    "    # è¿™æ˜¯ token çº§åˆ«çš„â€œè¿‘ä¼¼è´Ÿ KL é¡¹â€ï¼Œåé¢ä¼šåœ¨åºåˆ—çº§åˆ«åšå¹³å‡ã€‚\n",
    "    negative_approx_kl = log_prob - old_log_prob\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. è®¡ç®—â€œåºåˆ—çº§åˆ«â€çš„é‡è¦æ€§æ¯”ç‡ log s_i(Î¸)\n",
    "    # ------------------------------------------------------------------\n",
    "    # å…ˆè®¡ç®—æ¯ä¸ªåºåˆ—çš„é•¿åº¦ï¼ˆå³æœ‰æ•ˆ token æ•°é‡ï¼‰ï¼Œç”¨ response_mask æ’é™¤ paddingã€‚\n",
    "    # clamp(min=1) é˜²æ­¢å‡ºç°é•¿åº¦ä¸º 0 çš„æƒ…å†µå¯¼è‡´é™¤é›¶ã€‚\n",
    "    seq_lengths = torch.sum(response_mask, dim=-1).clamp(min=1)\n",
    "\n",
    "    # å¯¹æ¯ä¸ªåºåˆ—ï¼ŒæŠŠ (log Ï€_new - log Ï€_old) åœ¨æœ‰æ•ˆ token ä¸Šæ±‚å¹³å‡ï¼š\n",
    "    # negative_approx_kl_seq[i] â‰ˆ (1/|y_i|) * Î£_t (log Ï€_new - log Ï€_old)\n",
    "    # è¿™å¯¹åº”è®ºæ–‡é‡Œ log s_i(Î¸) çš„è¿‘ä¼¼å½¢å¼ã€‚\n",
    "    negative_approx_kl_seq = torch.sum(negative_approx_kl * response_mask, dim=-1) / seq_lengths\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. æŠŠâ€œåºåˆ—çº§åˆ«â€çš„æ¯”ç‡åˆ†é…åˆ°æ¯ä¸ª token ä¸Š\n",
    "    # ------------------------------------------------------------------\n",
    "    # ç†è®ºä¸Šï¼š\n",
    "    #   s_i,t(Î¸) = sg[s_i(Î¸)] * Ï€_Î¸(y_i,t|x,y_i,<t) / sg[Ï€_Î¸(y_i,t|x,y_i,<t)]\n",
    "    # åœ¨ log ç©ºé—´å¯å†™æˆï¼š\n",
    "    #   log s_i,t(Î¸) = sg[log s_i(Î¸)] + log_prob - sg[log_prob]\n",
    "    #\n",
    "    # è¿™é‡Œ negative_approx_kl_seq â‰ˆ log s_i(Î¸)ï¼Œ\n",
    "    # detach() ä¿è¯åœ¨åå‘ä¼ æ’­æ—¶ä¸ä¼šå¯¹ s_i(Î¸) å†æ±‚æ¢¯åº¦ï¼ˆåªè®© token çº§çš„ log_prob å‚ä¸ä¼˜åŒ–ï¼‰ã€‚\n",
    "    log_seq_importance_ratio = (\n",
    "        log_prob\n",
    "        - log_prob.detach()\n",
    "        + negative_approx_kl_seq.detach().unsqueeze(-1)  # [batch, 1] -> [batch, 1,] å¹¿æ’­åˆ° [batch, T]\n",
    "    )\n",
    "\n",
    "    # ä¸ºé˜²æ­¢ exp æº¢å‡ºï¼Œå¯¹ log æ¯”ç‡åšä¸€ä¸ªä¸Šç•Œæˆªæ–­ï¼ˆä¾‹å¦‚ â‰¤ 10ï¼‰\n",
    "    log_seq_importance_ratio = torch.clamp(log_seq_importance_ratio, max=10.0)  # æ•°å€¼ç¨³å®šæ€§\n",
    "\n",
    "    # ä» log ç©ºé—´è¿˜åŸåˆ°çœŸå®çš„æ¯”ç‡ï¼š\n",
    "    #   seq_importance_ratio = s_i,t(Î¸)\n",
    "    seq_importance_ratio = torch.exp(log_seq_importance_ratio)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. ç±»ä¼¼ PPO çš„è£å‰ªç­–ç•¥æ¢¯åº¦ç›®æ ‡\n",
    "    # ------------------------------------------------------------------\n",
    "    # æœªè£å‰ªçš„ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼š-A * r\n",
    "    pg_losses1 = -advantages * seq_importance_ratio\n",
    "\n",
    "    # è£å‰ªåçš„æ¯”ç‡ r_clip âˆˆ [1 - clip_ratio_low, 1 + clip_ratio_high]\n",
    "    pg_losses2 = -advantages * torch.clamp(\n",
    "        seq_importance_ratio,\n",
    "        1 - clip_ratio_low,\n",
    "        1 + clip_ratio_high,\n",
    "    )\n",
    "\n",
    "    # æŒ‰ PPO æ–¹å¼å–â€œæ‚²è§‚â€ç›®æ ‡ï¼šå¯¹æ¯ä¸ªä½ç½®å–ä¸¤è€…çš„æœ€å¤§å€¼\n",
    "    # ï¼ˆå› ä¸ºæŸå¤±æ˜¯è´Ÿçš„ï¼Œæœ€å¤§å€¼å¯¹åº”æ›´å°çš„æ”¶ç›Šï¼‰\n",
    "    pg_losses = torch.maximum(pg_losses1, pg_losses2)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. åœ¨åºåˆ—çº§åˆ«èšåˆæŸå¤±\n",
    "    # ------------------------------------------------------------------\n",
    "    # ä½¿ç”¨é€šç”¨çš„ agg_loss å‡½æ•°å¯¹ pg_losses åšèšåˆï¼š\n",
    "    #   - ä½¿ç”¨ response_mask æ’é™¤ padding / éå“åº” token\n",
    "    #   - æŒ‰ loss_agg_modeï¼ˆå¦‚ seq-mean-token-meanï¼‰åœ¨åºåˆ—å’Œ batch ç»´åº¦ä¸Šåšå¹³å‡\n",
    "    pg_loss = agg_loss(\n",
    "        loss_mat=pg_losses,\n",
    "        loss_mask=response_mask,\n",
    "        loss_agg_mode=loss_agg_mode,\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. ç»Ÿè®¡ä¸€äº› PPO ç›¸å…³çš„ç›‘æ§æŒ‡æ ‡\n",
    "    # ------------------------------------------------------------------\n",
    "    # pg_clipfracï¼šæœ‰å¤šå°‘ä½ç½®ä½¿ç”¨äº†è£å‰ªåçš„ç›®æ ‡ï¼ˆå³ pg_losses2 > pg_losses1ï¼‰\n",
    "    pg_clipfrac = verl_F.masked_mean(\n",
    "        torch.gt(pg_losses2, pg_losses1).float(),\n",
    "        response_mask,\n",
    "    )\n",
    "\n",
    "    # ä¸ºäº†å…¼å®¹å…¶ä»–æ¥å£ï¼Œè¿™é‡Œè¿”å›ä¸€ä¸ªå›ºå®šä¸º 0 çš„â€œä¸‹ç•Œè£å‰ªæ¯”ä¾‹â€\n",
    "    pg_clipfrac_lower = torch.tensor(0.0, device=pg_loss.device)\n",
    "\n",
    "    # è¿‘ä¼¼ KLï¼š-E[log Ï€_new - log Ï€_old]ï¼Œåªåœ¨å“åº”ä½ç½®ä¸Šå–å¹³å‡\n",
    "    ppo_kl = verl_F.masked_mean(-negative_approx_kl, response_mask)\n",
    "\n",
    "    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Positional Embedding, RoPEï¼‰çš„æœ€å°å®ç°ã€‚\n",
    "\n",
    "    æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "    - å°†æœ€åä¸€ä¸ªç‰¹å¾ç»´ head_dim æ‹†æˆä¸¤ä¸¤ä¸€ç»„ï¼ˆå¶æ•°/å¥‡æ•°ç´¢å¼•ï¼‰ï¼Œæ¯å¯¹ (x_even, x_odd)\n",
    "      è§†ä½œäºŒç»´å‘é‡ï¼ŒæŒ‰ä½ç½®ç›¸å…³çš„è§’åº¦ Î¸ è¿›è¡ŒäºŒç»´æ—‹è½¬ï¼š\n",
    "          [x_even', x_odd'] = [x_even, x_odd] Â· [[cosÎ¸, -sinÎ¸],\n",
    "                                                [sinÎ¸,  cosÎ¸]]\n",
    "    - Î¸ çš„é¢‘ç‡éšç»´åº¦è€Œå˜ï¼šÎ¸_t,i = t / base^(2i/d)ï¼Œt ä¸º token ä½ç½®ï¼Œi ä¸ºç»´åº¦å¯¹çš„ç´¢å¼•ã€‚\n",
    "\n",
    "    çº¦å®šçš„è¾“å…¥è¾“å‡ºå½¢çŠ¶ï¼š\n",
    "    - æœŸæœ› q, k å½¢çŠ¶ä¸º [batch, num_heads, seq_len, head_dim]ï¼Œä¸” head_dim ä¸ºå¶æ•°ã€‚\n",
    "      å¦‚ä½ çš„å¼ é‡ä¸º [batch, seq_len, num_heads, head_dim]ï¼Œå¯å…ˆ permute åˆ°è¯¥å½¢çŠ¶ï¼š\n",
    "          x = x.permute(0, 2, 1, 3)  # [B, S, H, D] -> [B, H, S, D]\n",
    "    - forward è¿”å›ä¸è¾“å…¥åŒå½¢çŠ¶çš„å¼ é‡ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        dim (int): æ¯ä¸ªå¤´çš„ç»´åº¦ head_dimï¼Œå¿…é¡»ä¸ºå¶æ•°ã€‚\n",
    "        base (float): é¢‘ç‡åŸºæ•°ï¼Œå¸¸ç”¨ 10000.0ï¼ˆLLaMA ç­‰ä¹Ÿå¯ç”¨ 10000ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"RoPE è¦æ±‚ head_dim ä¸ºå¶æ•°\"\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "\n",
    "        # inv_freq[i] = 1 / base^(2i/d), i=0..(d/2-1)\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        # æ³¨å†Œä¸º bufferï¼Œå‚ä¸è®¾å¤‡/ç²¾åº¦è¿ç§»ï¼Œä½†ä¸ä½œä¸ºå¯è®­ç»ƒå‚æ•°\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _build_sin_cos(self, seq_len: int, device, dtype, offset: int = 0):\n",
    "        \"\"\"\n",
    "        æ„é€ ç»™å®šåºåˆ—é•¿åº¦çš„ sin/cos è¡¨ï¼ˆå¹¿æ’­åˆ° dim ç»´ï¼‰ï¼Œç”¨äºä¸è¾“å…¥é€å…ƒç´ ç›¸ä¹˜ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "            cos: [seq_len, dim]ï¼Œæ¨¡å¼ä¸º [c0, c0, c1, c1, ...]\n",
    "            sin: [seq_len, dim]ï¼Œæ¨¡å¼ä¸º [s0, s0, s1, s1, ...]\n",
    "        \"\"\"\n",
    "        # ä½ç½®ç´¢å¼•ï¼ˆå¯å¸¦ offsetï¼Œç”¨äº KV-Cache å¢é‡è§£ç ï¼‰\n",
    "        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype) + offset  # [seq_len]\n",
    "        # å¤–ç§¯å¾—åˆ° [seq_len, dim//2] çš„è§’åº¦çŸ©é˜µ\n",
    "        freqs = torch.outer(t, self.inv_freq)  # Î¸_{t,i}\n",
    "        # æ‰©å±•åˆ° dimï¼šæ¯ä¸ªé¢‘ç‡é‡å¤ä¸¤æ¬¡ï¼Œå¯¹åº”å¶/å¥‡ä½\n",
    "        cos = torch.cos(freqs).repeat_interleave(2, dim=-1).to(dtype=dtype)  # [seq_len, dim]\n",
    "        sin = torch.sin(freqs).repeat_interleave(2, dim=-1).to(dtype=dtype)  # [seq_len, dim]\n",
    "        return cos, sin\n",
    "\n",
    "    @staticmethod\n",
    "    def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å°†æœ€åä¸€ç»´æ‹†æˆå¶/å¥‡ä¸¤éƒ¨åˆ†å¹¶åšäºŒç»´æ—‹è½¬æ‰€éœ€çš„ (x_even, x_odd) -> (-x_odd, x_even) å˜æ¢ã€‚\n",
    "        å½¢çŠ¶ä¸å˜ï¼Œé€‚é…å¹¿æ’­ã€‚\n",
    "        \"\"\"\n",
    "        x_even = x[..., ::2]  # å¶æ•°ä½\n",
    "        x_odd  = x[..., 1::2]  # å¥‡æ•°ä½\n",
    "        # å †å æˆ [.., dim//2, 2]ï¼Œåº”ç”¨æ—‹è½¬æ‰€éœ€çš„äº¤æ¢ä¸å–è´Ÿï¼Œå†è¿˜åŸåˆ° [..., dim]\n",
    "        x_rot = torch.stack((-x_odd, x_even), dim=-1).reshape_as(x)\n",
    "        return x_rot\n",
    "\n",
    "    def apply_rotary(self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å¯¹å¼ é‡ x åº”ç”¨ RoPEã€‚è¦æ±‚ï¼š\n",
    "          - x å½¢çŠ¶ä¸º [B, H, S, D]ï¼ˆåºåˆ—ç»´åœ¨å€’æ•°ç¬¬äºŒç»´ï¼Œç‰¹å¾ç»´åœ¨æœ€åä¸€ç»´ï¼‰\n",
    "          - cos/sin å½¢çŠ¶ä¸º [S, D]ï¼Œä¼šè‡ªåŠ¨å¹¿æ’­ä¸º [1,1,S,D]\n",
    "        \"\"\"\n",
    "        # å¹¿æ’­åˆ° [B, H, S, D]\n",
    "        cos = cos.unsqueeze(0).unsqueeze(0)  # [1,1,S,D]\n",
    "        sin = sin.unsqueeze(0).unsqueeze(0)  # [1,1,S,D]\n",
    "        # æ—‹è½¬ï¼šx' = x * cos + rotate_half(x) * sin\n",
    "        return x * cos + self._rotate_half(x) * sin\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, offset: int = 0):\n",
    "        \"\"\"\n",
    "        å¯¹ (q, k) åŒæ­¥åº”ç”¨ RoPEã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            q, k: [batch, num_heads, seq_len, head_dim]\n",
    "            offset: ä½ç½®åç§»ï¼ˆå¢é‡è§£ç æ—¶ï¼Œç­‰äºå·²ç¼“å­˜çš„åºåˆ—é•¿åº¦ï¼‰\n",
    "\n",
    "        è¿”å›:\n",
    "            q_rot, k_rot: ä¸è¾“å…¥åŒå½¢çŠ¶\n",
    "        \"\"\"\n",
    "        assert q.shape == k.shape, \"q/k å½¢çŠ¶éœ€ä¸€è‡´\"\n",
    "        B, H, S, D = q.shape\n",
    "        assert D == self.dim, f\"æœ€åä¸€ç»´ head_dim={D} ä¸åˆå§‹åŒ– dim={self.dim} ä¸ä¸€è‡´\"\n",
    "\n",
    "        cos, sin = self._build_sin_cos(S, q.device, q.dtype, offset=offset)\n",
    "        q_rot = self.apply_rotary(q, cos, sin)\n",
    "        k_rot = self.apply_rotary(k, cos, sin)\n",
    "        return q_rot, k_rot\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¯åˆ é™¤ï¼‰\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    B, H, S, D = 2, 4, 8, 64  # batch, heads, seq_len, head_dim\n",
    "    rope = RoPE(dim=D, base=10000.0)\n",
    "\n",
    "    q = torch.randn(B, H, S, D, dtype=torch.float32)\n",
    "    k = torch.randn(B, H, S, D, dtype=torch.float32)\n",
    "\n",
    "    # ç¬¬ä¸€æ¬¡æ•´åºåˆ—ç¼–ç ï¼ˆoffset=0ï¼‰\n",
    "    q_rot, k_rot = rope(q, k, offset=0)\n",
    "    print(q_rot.shape, k_rot.shape)  # torch.Size([2, 4, 8, 64]) torch.Size([2, 4, 8, 64])\n",
    "\n",
    "    # å¢é‡è§£ç æ—¶ï¼šå‡è®¾å·²ç¼“å­˜é•¿åº¦ä¸º Sï¼ˆoffset=Sï¼‰ï¼Œæ–°æ¥ 1 ä¸ª token\n",
    "    q_new = torch.randn(B, H, 1, D)\n",
    "    k_new = torch.randn(B, H, 1, D)\n",
    "    q_rot_new, k_rot_new = rope(q_new, k_new, offset=S)\n",
    "    print(q_rot_new.shape, k_rot_new.shape)  # torch.Size([2, 4, 1, 64]) torch.Size([2, 4, 1, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, lr=0.01):\n",
    "        # æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡å’Œåç½®ï¼ˆå¯¹åº”çº¿æ€§å±‚å‚æ•°ï¼‰\n",
    "        self.W = torch.randn(input_dim, 1, requires_grad=False) * 0.01\n",
    "        self.b = torch.zeros(1, requires_grad=False)\n",
    "        self.lr = lr  # å­¦ä¹ ç‡\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"å‰å‘ä¼ æ’­ï¼šè®¡ç®—çº¿æ€§è¾“å‡ºå¹¶åº”ç”¨Sigmoid\"\"\"\n",
    "        linear = torch.mm(X, self.W) + self.b  # X.shape: (n_samples, input_dim)\n",
    "        return torch.sigmoid(linear)  # è¾“å‡ºæ¦‚ç‡ P(y=1|x)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"è®¡ç®—äºŒå…ƒäº¤å‰ç†µæŸå¤±\"\"\"\n",
    "        epsilon = 1e-8  # é˜²æ­¢log(0)\n",
    "        return -torch.mean(y_true * torch.log(y_pred + epsilon) + \n",
    "                          (1 - y_true) * torch.log(1 - y_pred + epsilon))\n",
    "\n",
    "    def manual_update(self, X, y):\n",
    "        \"\"\"æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ï¼ˆä¸ä¾èµ–optimizerï¼‰\"\"\"\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        \n",
    "        # åå‘ä¼ æ’­æ‰‹åŠ¨æ±‚å¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰\n",
    "        n_samples = X.size(0)\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦ dL/dy_pred\n",
    "        dL_dy_pred = (y_pred - y) / n_samples  # shape: (n_samples, 1)\n",
    "        \n",
    "        # è®¡ç®—æ¢¯åº¦ dL/dW å’Œ dL/db\n",
    "        dL_dW = torch.mm(X.t(), dL_dy_pred)    # X.T @ dL_dy_pred\n",
    "        dL_db = torch.sum(dL_dy_pred, dim=0)   # æ²¿æ ·æœ¬ç»´åº¦æ±‚å’Œ\n",
    "        \n",
    "        # å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰\n",
    "        self.W -= self.lr * dL_dW\n",
    "        self.b -= self.lr * dL_db\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"é¢„æµ‹ç±»åˆ«ï¼ˆ0æˆ–1ï¼‰\"\"\"\n",
    "        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—\n",
    "            proba = self.forward(X)\n",
    "            return (proba >= threshold).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2445662913.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    if pos_num==0 or neg_sum==0:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def auc_count(self,y_true,y_prob):\n",
    "    sorted_nums = torch.argsort(y_prob, descending=False)\n",
    "    y_prob = y_prob[sorted_nums]\n",
    "    y_true = y_true[sorted_nums]\n",
    "\n",
    "    pos_num = (y_true == 1).sum().item()\n",
    "    neg_num = (y_true == 0).sum().item()\n",
    "    \n",
    "    cnt_cum = 0\n",
    "    same_prob = None\n",
    "    same_prob_cnt = 0\n",
    "\n",
    "    for i, prob in enumerate(y_prob, start=1):\n",
    "        if prob != same_prob:\n",
    "            if same_prob_cnt > 0:\n",
    "                avg_prob = same_prob_cnt / same_prob_cnt\n",
    "                cnt_cum += avg_prob * same_prob_cnt\n",
    "\n",
    "                same_prob_cnt = 0\n",
    "                prob_sum = 0\n",
    "            same_prob = prob\n",
    "        if y_true[i-1] == 1:\n",
    "            same_prob_cnt += 1 \n",
    "            prob_sum += prob\n",
    "        \n",
    "    if same_prob_cnt>0:\n",
    "        avg_prob = prob_sum / same_prob_cnt\n",
    "        cnt_cum += avg_prob * same_prob_cnt\n",
    "\n",
    "    auc = (cnt_cum - pos_num*(pos_num+1)/2)/(pos_num * neg_num)\n",
    "\n",
    "    return auc \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_probs = y_prob[y_true == 1]\n",
    "neg_probs = y_prob[y_true == 0]\n",
    "comparison = pos_probs[:, None] > neg_probs[None, :]\n",
    "auc = (comparison.sum() + 0.5 * (pos_probs[:, None] == neg_probs[None, :]).sum()) / (pos_num * neg_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUCè®¡ç®—ä»£ç  è¡¨ç¤ºæ­£æ ·æœ¬é¢„æµ‹æ¦‚ç‡å¤§äºè´Ÿæ ·æœ¬é¢„æµ‹æ¦‚ç‡çš„æ¦‚ç‡ï¼Œè¯„ä¼°äºŒåˆ†ç±»æ¨¡å‹å¯¹æ­£è´Ÿæ ·æœ¬çš„æ’åºèƒ½åŠ›ï¼Œåæ˜ åŒºåˆ†ä¸¤ç±»æ ·æœ¬çš„æ•ˆæœ\n",
    "\n",
    "# æŒ‰é¢„æµ‹æ¦‚ç‡å‡åºæ’åˆ—ï¼Œå¹¶åŒæ­¥è°ƒæ•´çœŸå®æ ‡ç­¾é¡ºåº\n",
    "def auc_count(self, y_prob, y_true):\n",
    "    sorted_nums = torch.argsort(y_prob, descending=False)\n",
    "    y_true_sorted = y_true[sorted_nums]\n",
    "    y_prob_sorted = y_prob[sorted_nums]\n",
    "\n",
    "    pos_num = (y_true == 1).sum().item()\n",
    "    neg_num = (y_true == 0).sum().item()\n",
    "\n",
    "    if pos_num == 0 or neg_num == 0:\n",
    "        return 0.0\n",
    "    # è®¡ç®—æ¯ä¸ªæ­£æ ·æœ¬çš„ç´¯è®¡æ’åï¼ˆå¤„ç†ç›¸åŒæ¦‚ç‡çš„æƒ…å†µï¼‰\n",
    "    cum_rank = 0.0\n",
    "    cur_prob = None\n",
    "    same_prob_count = 0\n",
    "    same_rank_sum = 0\n",
    "\n",
    "    for idx, prob in enumerate(y_prob_sorted, start = 1):\n",
    "        if prob != cur_prob:\n",
    "            if same_prob_count > 0:\n",
    "                avg_rank = same_rank_sum / same_prob_count\n",
    "                cum_rank += avg_rank * same_prob_count\n",
    "                \n",
    "                same_prob_count = 0\n",
    "                same_rank_sum = 0.0\n",
    "            cur_prob = prob\n",
    "        #åªæ‰¾æ­£æ ·æœ¬çš„æ’å\n",
    "        if y_true_sorted[idx - 1] == 1:  \n",
    "            same_prob_count += 1\n",
    "            same_rank_sum += idx\n",
    "    # å¤„ç†æœ€åä¸€ç»„ç›¸åŒæ¦‚ç‡çš„æƒ…å†µ\n",
    "    if same_prob_count > 0:\n",
    "        avg_rank = same_rank_sum/ same_prob_count\n",
    "        cum_rank += avg_rank * same_prob_count\n",
    "    # åº”ç”¨AUCè®¡ç®—å…¬å¼\n",
    "    auc = (cum_rank - pos_num*(pos_num+1)/2)/(pos_num * neg_num)  \n",
    "    return auc      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class KLDivergence(nn.Module):\n",
    "    \"\"\"\n",
    "    KL æ•£åº¦ï¼ˆKullbackâ€“Leibler Divergenceï¼‰è®¡ç®—æ¨¡å—\n",
    "\n",
    "    KL(P || Q) = sum_x P(x) * log(P(x) / Q(x))\n",
    "\n",
    "    ä¸€èˆ¬ç”¨é€”ï¼š\n",
    "      - åº¦é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ P å’Œ Q ä¹‹é—´çš„â€œå·®å¼‚/ä¸ä¸€è‡´åº¦â€\n",
    "      - å˜åˆ†è‡ªç¼–ç å™¨(VAE)ä¸­çº¦æŸéšå˜é‡åˆ†å¸ƒ q(z|x) æ¥è¿‘å…ˆéªŒ p(z)\n",
    "      - è’¸é¦ï¼ˆdistillationï¼‰ä¸­è®©å­¦ç”Ÿæ¨¡å‹åˆ†å¸ƒæ¥è¿‘è€å¸ˆæ¨¡å‹åˆ†å¸ƒ\n",
    "      - RL ä¸­çº¦æŸæ–°æ—§ç­–ç•¥åˆ†å¸ƒä¸è¦å·®å¤ªå¤šï¼ˆå¦‚ PPO/GSPO ä¸­çš„ KL ç›‘æ§ï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "            epsilon: é˜²æ­¢å‡ºç° log(0) / é™¤é›¶ çš„æå°å€¼\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, P_logits: torch.Tensor, Q_logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        è¾“å…¥æ˜¯ logitsï¼ˆæœªå½’ä¸€åŒ–åˆ†æ•°ï¼‰ï¼Œå†…éƒ¨è½¬æˆæ¦‚ç‡åè®¡ç®— KL(P || Q)ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "            P_logits: ä½œä¸ºâ€œçœŸå®/ç›®æ ‡â€åˆ†å¸ƒçš„ logitsï¼Œå½¢çŠ¶ [..., num_classes]\n",
    "            Q_logits: ä½œä¸ºâ€œè¿‘ä¼¼/é¢„æµ‹â€åˆ†å¸ƒçš„ logitsï¼Œå½¢çŠ¶ [..., num_classes]\n",
    "\n",
    "        è¿”å›:\n",
    "            æ ‡é‡ KL æŸå¤±ï¼ˆbatch/æ ·æœ¬å¹³å‡ï¼‰\n",
    "        \"\"\"\n",
    "        # å°† logits è½¬æˆæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶åŠ ä¸Š epsilon æé«˜æ•°å€¼ç¨³å®šæ€§\n",
    "        P = F.softmax(P_logits, dim=-1)\n",
    "        Q = F.softmax(Q_logits, dim=-1)\n",
    "\n",
    "        P = P + self.epsilon\n",
    "        Q = Q + self.epsilon\n",
    "\n",
    "        # è®¡ç®—å¯¹æ•°æ¦‚ç‡\n",
    "        logP = torch.log(P)\n",
    "        logQ = torch.log(Q)\n",
    "\n",
    "        # KL(P||Q) = sum P * (logP - logQ)\n",
    "        kl_elements = P * (logP - logQ)          # [..., num_classes]\n",
    "        kl_per_sample = kl_elements.sum(dim=-1)  # [...] æ¯ä¸ªæ ·æœ¬çš„ KL\n",
    "\n",
    "        # è¿”å›æ‰€æœ‰æ ·æœ¬çš„å¹³å‡\n",
    "        return kl_per_sample.mean()\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    äºŒå…ƒäº¤å‰ç†µï¼ˆBinary Cross Entropy, BCEï¼‰æŸå¤±æ¨¡å—\n",
    "\n",
    "    å¯¹åº”å…¬å¼ï¼ˆå•æ ·æœ¬ï¼‰:\n",
    "      y âˆˆ {0,1}, p = Ïƒ(z) ä¸ºé¢„æµ‹ä¸ºæ­£ç±»çš„æ¦‚ç‡\n",
    "      BCE(y, p) = -[ y * log(p) + (1 - y) * log(1 - p) ]\n",
    "\n",
    "    ä¸€èˆ¬ç”¨é€”ï¼š\n",
    "      - äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚åƒåœ¾é‚®ä»¶è¯†åˆ«ï¼Œç‚¹å‡»ç‡é¢„ä¼°ï¼‰\n",
    "      - æ¯ä¸ªè¾“å‡ºç‹¬ç«‹åšäºŒåˆ†ç±»çš„å¤šæ ‡ç­¾ä»»åŠ¡ï¼ˆmulti-labelï¼‰\n",
    "      - Logistic Regression / çº¿æ€§æ¨¡å‹+sigmoid çš„æ ‡å‡†æŸå¤±\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "            epsilon: é˜²æ­¢ log(0) çš„æå°å€¼\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "            logits: æ¨¡å‹çš„åŸå§‹è¾“å‡ºï¼ˆæœªè¿‡ sigmoidï¼‰ï¼Œå½¢çŠ¶ [batch_size, 1] æˆ– [batch_size]\n",
    "            targets: çœŸå®æ ‡ç­¾ï¼Œå–å€¼ 0 æˆ– 1ï¼Œå½¢çŠ¶ä¸ logits å¯å¹¿æ’­ä¸€è‡´\n",
    "\n",
    "        è¿”å›:\n",
    "            æ ‡é‡ BCE æŸå¤±ï¼ˆbatch å¹³å‡ï¼‰\n",
    "        \"\"\"\n",
    "        # å…ˆé€šè¿‡ sigmoid æŠŠ logits æ˜ å°„åˆ° (0,1) æ¦‚ç‡åŒºé—´\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # è£å‰ªæ¦‚ç‡ï¼Œé¿å… log(0)\n",
    "        probs = torch.clamp(probs, self.epsilon, 1.0 - self.epsilon)\n",
    "\n",
    "        # æŒ‰å…¬å¼è®¡ç®—é€æ ·æœ¬æŸå¤±\n",
    "        # loss_i = -[y_i * log(p_i) + (1 - y_i) * log(1 - p_i)]\n",
    "        loss = -(\n",
    "            targets * torch.log(probs) +\n",
    "            (1.0 - targets) * torch.log(1.0 - probs)\n",
    "        )\n",
    "\n",
    "        # å¯¹æ‰€æœ‰æ ·æœ¬å–å¹³å‡ï¼Œå¾—åˆ°æ ‡é‡æŸå¤±\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(self, P, Q, epsilon = 1e-8):\n",
    "    P += epsilon\n",
    "    Q += epsilon\n",
    "\n",
    "    P = F.softmax(P,dim=-1)\n",
    "    Q = F.softmax(Q,dim=-1)\n",
    "\n",
    "    logP = torch.log(P)\n",
    "    logQ = torch.log(Q)\n",
    "\n",
    "    KL_elements = P *(logP - logQ)\n",
    "    P/Q - logp - logQ - 1\n",
    "    KL = KL_elements.sum(dim=-1)\n",
    "\n",
    "    return KL.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: tensor([[0.0469, 0.0489, 0.2302, 0.0418, 0.0568, 0.0148, 0.0487, 0.2712, 0.2014,\n",
      "         0.0392],\n",
      "        [0.0436, 0.3497, 0.0228, 0.0051, 0.0119, 0.0270, 0.0196, 0.0320, 0.4057,\n",
      "         0.0826],\n",
      "        [0.1846, 0.1609, 0.1473, 0.0572, 0.2800, 0.0317, 0.0273, 0.0310, 0.0408,\n",
      "         0.0393],\n",
      "        [0.0768, 0.0925, 0.2772, 0.1169, 0.0447, 0.0567, 0.1478, 0.0679, 0.1017,\n",
      "         0.0178]])\n",
      "P1: tensor([[0.0469, 0.0489, 0.2302, 0.0418, 0.0568, 0.0148, 0.0487, 0.2712, 0.2014,\n",
      "         0.0392],\n",
      "        [0.0436, 0.3497, 0.0228, 0.0051, 0.0119, 0.0270, 0.0196, 0.0320, 0.4057,\n",
      "         0.0826],\n",
      "        [0.1846, 0.1609, 0.1473, 0.0572, 0.2800, 0.0317, 0.0273, 0.0310, 0.0408,\n",
      "         0.0393],\n",
      "        [0.0768, 0.0925, 0.2772, 0.1169, 0.0447, 0.0567, 0.1478, 0.0679, 0.1017,\n",
      "         0.0178]])\n",
      "tensor(1.0280)\n"
     ]
    }
   ],
   "source": [
    "def kl(P,Q,epsilon = 1e-8):\n",
    "    P += epsilon\n",
    "    Q += epsilon\n",
    "\n",
    "    P = F.softmax(P,dim=-1)\n",
    "    Q = F.softmax(Q, dim=-1)\n",
    "\n",
    "    logP = torch.log(P)\n",
    "    logQ = torch.log(Q)\n",
    "\n",
    "    KL = P * (logP - logQ)\n",
    "    KL = KL.sum(dim=-1)\n",
    "\n",
    "    return KL.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLæ•£åº¦\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kl_divergence_manual(P, Q, epsilon=1e-8):\n",
    "    \"\"\"æ‰‹åŠ¨å®ç°KLæ•£åº¦è®¡ç®—\"\"\"\n",
    "    # æ·»åŠ æå°å€¼é¿å…log(0)å¯¼è‡´æ•°å€¼ä¸ç¨³å®š\n",
    "    P = P + epsilon\n",
    "    Q = Q + epsilon\n",
    "    \n",
    "    # å½’ä¸€åŒ–å¤„ç†ï¼ˆè‹¥è¾“å…¥ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼‰\n",
    "    # æ²¿æœ€åä¸€ä¸ªç»´åº¦æ±‚å’Œå¹¶ä¿æŒç»´åº¦ï¼Œå°†å¼ é‡è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "    P = P / P.sum(dim=-1, keepdim=True)\n",
    "    Q = Q / Q.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼ˆKLæ•£åº¦æ ¸å¿ƒæ“ä½œï¼‰\n",
    "    logP = torch.log(P)\n",
    "    logQ = torch.log(Q)\n",
    "    \n",
    "    # é€å…ƒç´ è®¡ç®—KLæ•£åº¦ï¼šP * (logP - logQ)\n",
    "    kl_elements = P * (logP - logQ)\n",
    "    \n",
    "    # æ²¿æœ€åä¸€ä¸ªç»´åº¦æ±‚å’Œï¼ˆå¯¹æ¯ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒæ±‚å’Œï¼‰\n",
    "    kl = kl_elements.sum(dim=-1)\n",
    "    \n",
    "    # è¿”å›æ‰¹æ¬¡å¹³å‡å€¼\n",
    "    return kl.mean()\n",
    "\n",
    "def kl_divergence_pytorch(P_logits, Q_logits):\n",
    "    \"\"\"ä½¿ç”¨PyTorchå†…ç½®å‡½æ•°è®¡ç®—KLæ•£åº¦\"\"\"\n",
    "    # å¯¹Qçš„logitsåº”ç”¨log_softmaxå¾—åˆ°å¯¹æ•°æ¦‚ç‡ï¼ˆç¬¦åˆKLDivLossè¾“å…¥è¦æ±‚ï¼‰\n",
    "    log_Q = F.log_softmax(Q_logits, dim=-1)\n",
    "    \n",
    "    # å¯¹Pçš„logitsåº”ç”¨softmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼ˆç¬¦åˆKLDivLossç›®æ ‡è¦æ±‚ï¼‰\n",
    "    P = F.softmax(P_logits, dim=-1)\n",
    "    \n",
    "    # åˆå§‹åŒ–KLæ•£åº¦æŸå¤±å‡½æ•°ï¼Œreduction='batchmean'è¡¨ç¤ºæŒ‰æ‰¹æ¬¡å‡å€¼è¿”å›\n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    # è®¡ç®—KLæ•£åº¦ï¼ˆè¾“å…¥log_Qéœ€åœ¨å‰ï¼Œç›®æ ‡Påœ¨åï¼‰\n",
    "    loss = kl_loss(log_Q, P)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTæ¨¡å‹æ ¸å¿ƒç»“æ„ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.segment_emb = nn.Embedding(2, hidden_size)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 512, hidden_size))\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        # åµŒå…¥èåˆ\n",
    "        token_emb = self.embeddings(input_ids)\n",
    "        seg_emb = self.segment_emb(segment_ids)\n",
    "        pos_emb = self.pos_emb[:, :input_ids.size(1), :]\n",
    "        x = token_emb + seg_emb + pos_emb\n",
    "        # ç¼–ç å™¨å †å \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLMï¼ˆMasked Language Modelï¼‰ï¼šéšæœºé®è”½15%çš„tokenï¼ˆ80%æ›¿æ¢ä¸º[MASK]ï¼Œ10%éšæœºæ›¿æ¢ï¼Œ10%ä¿ç•™åŸè¯ï¼‰\n",
    "class MaskedLM(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, hidden_states, masked_positions):\n",
    "        # æå–è¢«é®è”½ä½ç½®çš„éšè—çŠ¶æ€\n",
    "        batch_size, seq_len, dim = hidden_states.shape\n",
    "        flat_positions = masked_positions.view(-1)\n",
    "        selected = hidden_states.view(-1, dim)[flat_positions]\n",
    "        # è§£ç é¢„æµ‹\n",
    "        x = F.gelu(self.dense(selected))\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.decoder(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#äº¤å‰ç†µæŸå¤±  -P(x)logQ(x) Pä¸ºçœŸå®åˆ†å¸ƒï¼ŒQä¸ºé¢„æµ‹åˆ†å¸ƒ\n",
    "import torch\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: æ¨¡å‹åŸå§‹è¾“å‡º (æœªå½’ä¸€åŒ–), shape: (N, C)\n",
    "    labels: çœŸå®æ ‡ç­¾ï¼ˆç±»åˆ«ç´¢å¼•ï¼‰, shape: (N,)\n",
    "    \"\"\"\n",
    "    # æ•°å€¼ç¨³å®šæ€§å¤„ç†: å‡å»æœ€å¤§å€¼é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸\n",
    "    logits = logits - torch.max(logits, dim=1, keepdim=True)[0]\n",
    "    \n",
    "    # è®¡ç®—LogSoftmax (ç­‰ä»·äº log(softmax(logits)))\n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_probs = logits - torch.log(torch.sum(exp_logits, dim=1, keepdim=True))\n",
    "    \n",
    "    # æå–çœŸå®ç±»åˆ«å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡\n",
    "    n_samples = logits.shape[0]\n",
    "    true_class_logprobs = log_probs[range(n_samples), labels]\n",
    "    \n",
    "    # è®¡ç®—æŸå¤±ï¼ˆå¹³å‡æŸå¤±ï¼‰\n",
    "    loss = -torch.mean(true_class_logprobs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (984388660.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class Lora(nn.modules):\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Lora(nn.modules):\n",
    "    def __init__(self, in_dim, out_dim, rank=8, alpha = 16):\n",
    "        super().__init__\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "\n",
    "        self.scalling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scalling\n",
    "\n",
    "        return x @ (self.original_weight + delta_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.in_dim = original_layer.in_features\n",
    "        self.out_dim = original_layer.out_features\n",
    "\n",
    "        self.original_weight = original_layer.weight.clone().detach()  # å…‹éš†åŸå§‹æƒé‡å¹¶ç¦æ­¢æ¢¯åº¦æ›´æ–°\n",
    "        self.original_weight.requires_grad_(False)\n",
    "        # ç¼©æ”¾å› å­ï¼šalpha / rankï¼Œç”¨äºå¹³è¡¡ä½ç§©æ›´æ–°çš„å¼ºåº¦\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(self.in_dim, rank) * 0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, self.out_dim))\n",
    "\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        # è®¡ç®—ä½ç§©æ›´æ–° Î”W = A @ B * scaling\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scaling\n",
    "        # å°†æ›´æ–°é‡å åŠ åˆ°åŸå§‹æƒé‡ï¼ˆéœ€ä»å¤–éƒ¨ä¼ å…¥åŸå§‹æƒé‡ï¼‰\n",
    "        combined_weight = self.original_weight + delta_W\n",
    "        return F.linear(x, combined_weight, self.original_layer.bias)\n",
    "    \n",
    "original_linear = nn.Linear(768, 512)\n",
    "\n",
    "    # åˆ›å»ºLoRAé€‚é…å±‚\n",
    "lora_layer = LoRALayer(original_linear, rank=8, alpha=16)\n",
    "\n",
    "    # å‰å‘ä¼ æ’­æ—¶ç›´æ¥æ›¿æ¢ä½¿ç”¨\n",
    "x = torch.randn(32, 768)  # è¾“å…¥ç»´åº¦\n",
    "output = lora_layer(x)    # è¾“å‡ºç»´åº¦ (32, 512)\n",
    "\n",
    "\n",
    "class Lora(nn.Module):\n",
    "    def __init__(self, original_layer, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.input_dim = original_layer.in_features\n",
    "        self.out_dim = original_layer.out_features\n",
    "\n",
    "        self.original_weight = original_layer.weight.clone().detach()\n",
    "        self.original_weight.require_grad_(False)\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(self.input_dim, rank) *0.02)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, self.out_dim))\n",
    "\n",
    "        self.scaling = alpha / rank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        delta_W = torch.matmul(self.A, self.B) * self.scaling\n",
    "        combined_weight = self.original_weight + delta_W\n",
    "        return F.linear(x, combined_weight, self.original_weight.bias)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNormä»£ç \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # å¯å­¦ä¹ å‚æ•°Î³\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) #å€’æ•°å¹³æ–¹æ ¹\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1838695233.py, line 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 112\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"ä¸“å®¶ç½‘ç»œæ¨¡å—\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"MoEæ ¸å¿ƒå±‚\"\"\"\n",
    "    def __init__(self, d_model=768, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # ä¸“å®¶æ± åˆå§‹åŒ–\n",
    "        self.experts = nn.ModuleList([Expert(d_model, d_model*4) for _ in range(num_experts)])\n",
    "        \n",
    "        # é—¨æ§ç½‘ç»œ\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "        \n",
    "        # è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±ç³»æ•°\n",
    "        self.aux_loss_coef = 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # é—¨æ§ç½‘ç»œè®¡ç®—\n",
    "        logits = self.gate(x)  # [B, S, E]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Top-Kä¸“å®¶é€‰æ‹©\n",
    "        topk_probs, topk_indices = probs.topk(self.top_k, dim=-1)  # [B, S, K]\n",
    "        topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # ä¸“å®¶è®¡ç®—ä¸ç»„åˆ\n",
    "        outputs = torch.zeros_like(x)   #åˆ›å»ºå½¢å¼ç›¸åŒçš„å…¨0å¼ é‡\n",
    "        for i in range(self.num_experts):\n",
    "            # åˆ›å»ºå½“å‰ä¸“å®¶çš„mask\n",
    "            expert_mask = (topk_indices == i)\n",
    "            if expert_mask.any():\n",
    "                # å½“å‰ä¸“å®¶å¤„ç†çš„tokenç´¢å¼•\n",
    "                batch_idx, seq_idx = torch.where(expert_mask)\n",
    "                \n",
    "                # è·å–å¯¹åº”è¾“å…¥å¹¶è®¡ç®—ä¸“å®¶è¾“å‡º\n",
    "                expert_input = x[batch_idx, seq_idx]\n",
    "                expert_output = self.experts[i](expert_input)\n",
    "                \n",
    "                # åŠ æƒç´¯åŠ ç»“æœ\n",
    "                weights = topk_probs[batch_idx, seq_idx, expert_mask[expert_mask]]\n",
    "                outputs[batch_idx, seq_idx] += expert_output * weights.unsqueeze(-1)\n",
    "        \n",
    "        # è´Ÿè½½å‡è¡¡æŸå¤±è®¡ç®—\n",
    "        expert_mask = F.one_hot(topk_indices, self.num_experts).float()\n",
    "        expert_usage = expert_mask.mean(dim=0).mean(dim=0)\n",
    "        aux_loss = (expert_usage.std() + 1e-6) * self.aux_loss_coef\n",
    "        \n",
    "        return outputs, aux_loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
